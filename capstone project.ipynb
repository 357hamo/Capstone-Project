{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 02.14.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Bringing It All Together\n",
    "\n",
    "In this lab, you will bring together many of the tools and techniques that you have learned throughout this course into a final project. You can choose from many different paths to get to the solution. You could use AWS Managed Services, such as Amazon Comprehend, or use the Amazon SageMaker models. Have fun on whichever path you choose.\n",
    "\n",
    "### Business scenario\n",
    "\n",
    "You work for a training organization that recently developed an introductory course about machine learning (ML). The course includes more than 40 videos that cover a broad range of ML topics. You have been asked to create an application that will students can use to quickly locate and view video content by searching for topics and key phrases.\n",
    "\n",
    "You have downloaded all of the videos to an Amazon Simple Storage Service (Amazon S3) bucket. Your assignment is to produce a dashboard that meets your supervisor’s requirements.\n",
    "\n",
    "To assist you, all of the previous labs have been provided in this workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab steps\n",
    "\n",
    "To complete this lab, you will follow these steps:\n",
    "\n",
    "1. [Viewing the video files](#1.-Viewing-the-video-files)\n",
    "2. [Transcribing the videos](#2.-Transcribing-the-videos)\n",
    "3. [Normalizing the text](#3.-Normalizing-the-text)\n",
    "4. [Extracting key phrases and topics](#4.-Extracting-key-phrases-and-topics)\n",
    "5. [Creating the dashboard](#5.-Creating-the-dashboard)\n",
    "\n",
    "## Submitting your work\n",
    "\n",
    "1. In the lab console, choose **Submit** to record your progress and when prompted, choose **Yes**.\n",
    "\n",
    "1. If the results don't display after a couple of minutes, return to the top of these instructions and choose **Grades**.\n",
    "\n",
    "     **Tip**: You can submit your work multiple times. After you change your work, choose **Submit** again. Your last submission is what will be recorded for this lab.\n",
    "\n",
    "1. To find detailed feedback on your work, choose **Details** followed by **View Submission Report**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful information\n",
    "\n",
    "The following cell contains some information that might be useful as you complete this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew\"\n",
    "job_data_access_role = 'arn:aws:iam::592996473068:role/service-role/c127808a3228856l7912640t1w-ComprehendDataAccessRole-UkuYFZmTzEbG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Viewing the video files\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source video files are located in the following shared Amazon Simple Storage Service (Amazon S3) bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-26 20:17:33  410925369 Mod01_Course Overview.mp4\r\n",
      "2021-04-26 20:10:02   39576695 Mod02_Intro.mp4\r\n",
      "2021-04-26 20:31:23  302994828 Mod02_Sect01.mp4\r\n",
      "2021-04-26 20:17:33  416563881 Mod02_Sect02.mp4\r\n",
      "2021-04-26 20:17:33  318685583 Mod02_Sect03.mp4\r\n",
      "2021-04-26 20:17:33  255877251 Mod02_Sect04.mp4\r\n",
      "2021-04-26 20:23:51   99988046 Mod02_Sect05.mp4\r\n",
      "2021-04-26 20:24:54   50700224 Mod02_WrapUp.mp4\r\n",
      "2021-04-26 20:26:27   60627667 Mod03_Intro.mp4\r\n",
      "2021-04-26 20:26:28  272229844 Mod03_Sect01.mp4\r\n",
      "2021-04-26 20:27:06  309127124 Mod03_Sect02_part1.mp4\r\n",
      "2021-04-26 20:27:06  195635527 Mod03_Sect02_part2.mp4\r\n",
      "2021-04-26 20:28:03  123924818 Mod03_Sect02_part3.mp4\r\n",
      "2021-04-26 20:31:28  171681915 Mod03_Sect03_part1.mp4\r\n",
      "2021-04-26 20:32:07  285200083 Mod03_Sect03_part2.mp4\r\n",
      "2021-04-26 20:33:17  105470345 Mod03_Sect03_part3.mp4\r\n",
      "2021-04-26 20:35:10  157185651 Mod03_Sect04_part1.mp4\r\n",
      "2021-04-26 20:36:27  187435635 Mod03_Sect04_part2.mp4\r\n",
      "2021-04-26 20:36:40  280720369 Mod03_Sect04_part3.mp4\r\n",
      "2021-04-26 20:40:01  443479313 Mod03_Sect05.mp4\r\n",
      "2021-04-26 20:40:08  234182186 Mod03_Sect06.mp4\r\n",
      "2021-04-26 20:40:33  207718047 Mod03_Sect07_part1.mp4\r\n",
      "2021-04-26 20:42:07  125592110 Mod03_Sect07_part2.mp4\r\n",
      "2021-04-26 20:45:10  508500301 Mod03_Sect07_part3.mp4\r\n",
      "2021-04-26 20:46:16  320126756 Mod03_Sect08.mp4\r\n",
      "2021-04-26 20:46:43   41839508 Mod03_WrapUp.mp4\r\n",
      "2021-04-26 20:46:55   34148489 Mod04_Intro.mp4\r\n",
      "2021-04-26 20:48:24   84959465 Mod04_Sect01.mp4\r\n",
      "2021-04-26 20:48:25  345182970 Mod04_Sect02_part1.mp4\r\n",
      "2021-04-26 20:51:34  218661651 Mod04_Sect02_part2.mp4\r\n",
      "2021-04-26 20:53:32  430140637 Mod04_Sect02_part3.mp4\r\n",
      "2021-04-26 20:56:03   22036605 Mod04_WrapUp.mp4\r\n",
      "2021-04-26 20:57:18   49187118 Mod05_Intro.mp4\r\n",
      "2021-04-26 20:58:19  245798071 Mod05_Sect01_ver2.mp4\r\n",
      "2021-04-26 20:58:50  233314835 Mod05_Sect02_part1_ver2.mp4\r\n",
      "2021-04-26 20:59:14  348545306 Mod05_Sect02_part2.mp4\r\n",
      "2021-04-26 20:59:17  239142711 Mod05_Sect03_part1.mp4\r\n",
      "2021-04-26 21:06:04  267533559 Mod05_Sect03_part2.mp4\r\n",
      "2021-04-26 21:06:06  212502220 Mod05_Sect03_part3.mp4\r\n",
      "2021-04-26 21:06:48  206317022 Mod05_Sect03_part4_ver2.mp4\r\n",
      "2021-04-26 21:06:48   60361230 Mod05_WrapUp_ver2.mp4\r\n",
      "2021-04-26 21:09:14   35397860 Mod06_Intro.mp4\r\n",
      "2021-04-26 21:09:24  845633599 Mod06_Sect01.mp4\r\n",
      "2021-04-26 21:10:47  326126684 Mod06_Sect02.mp4\r\n",
      "2021-04-26 21:12:26   19790740 Mod06_WrapUp.mp4\r\n",
      "2021-04-26 21:12:56  131249036 Mod07_Sect01.mp4\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transcribing the videos\n",
    " ([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to implement your solution to transcribe the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod01_Course Overview.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod01_Course Overview.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Intro.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Intro.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Sect05.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Sect05.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Sect01.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Sect01.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_WrapUp.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_WrapUp.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Intro.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Intro.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Sect03.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Sect03.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Sect04.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Sect04.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Sect02.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Sect02.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part3.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect02_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect02_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect01.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect01.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part1.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect03_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part1.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect02_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part3.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect03_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part1.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect04_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect03_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect04_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect07_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part3.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect04_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect08.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect08.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect05.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect05.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part3.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect07_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_Intro.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_Intro.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part1.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_Sect02_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect06.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect06.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_Sect02_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_Sect01.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_Sect01.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part1.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect07_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_WrapUp.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_WrapUp.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_WrapUp.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_WrapUp.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Intro.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Intro.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect02_part1_ver2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect02_part1_ver2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part1.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect03_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect01_ver2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect01_ver2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect02_part2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect02_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part3.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_Sect02_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod06_Intro.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod06_Intro.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_WrapUp_ver2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_WrapUp_ver2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part3.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect03_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part4_ver2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect03_part4_ver2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod06_WrapUp.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod06_WrapUp.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part2.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect03_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod07_Sect01.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod07_Sect01.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod06_Sect02.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod06_Sect02.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod06_Sect01.mp4 to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod06_Sect01.mp4\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/ s3://{bucket}/input/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input/Mod01_Course Overview.mp4\n",
      "input/Mod02_Intro.mp4\n",
      "input/Mod02_Sect01.mp4\n",
      "input/Mod02_Sect02.mp4\n",
      "input/Mod02_Sect03.mp4\n",
      "input/Mod02_Sect04.mp4\n",
      "input/Mod02_Sect05.mp4\n",
      "input/Mod02_WrapUp.mp4\n",
      "input/Mod03_Intro.mp4\n",
      "input/Mod03_Sect01.mp4\n",
      "input/Mod03_Sect02_part1.mp4\n",
      "input/Mod03_Sect02_part2.mp4\n",
      "input/Mod03_Sect02_part3.mp4\n",
      "input/Mod03_Sect03_part1.mp4\n",
      "input/Mod03_Sect03_part2.mp4\n",
      "input/Mod03_Sect03_part3.mp4\n",
      "input/Mod03_Sect04_part1.mp4\n",
      "input/Mod03_Sect04_part2.mp4\n",
      "input/Mod03_Sect04_part3.mp4\n",
      "input/Mod03_Sect05.mp4\n",
      "input/Mod03_Sect06.mp4\n",
      "input/Mod03_Sect07_part1.mp4\n",
      "input/Mod03_Sect07_part2.mp4\n",
      "input/Mod03_Sect07_part3.mp4\n",
      "input/Mod03_Sect08.mp4\n",
      "input/Mod03_WrapUp.mp4\n",
      "input/Mod04_Intro.mp4\n",
      "input/Mod04_Sect01.mp4\n",
      "input/Mod04_Sect02_part1.mp4\n",
      "input/Mod04_Sect02_part2.mp4\n",
      "input/Mod04_Sect02_part3.mp4\n",
      "input/Mod04_WrapUp.mp4\n",
      "input/Mod05_Intro.mp4\n",
      "input/Mod05_Sect01_ver2.mp4\n",
      "input/Mod05_Sect02_part1_ver2.mp4\n",
      "input/Mod05_Sect02_part2.mp4\n",
      "input/Mod05_Sect03_part1.mp4\n",
      "input/Mod05_Sect03_part2.mp4\n",
      "input/Mod05_Sect03_part3.mp4\n",
      "input/Mod05_Sect03_part4_ver2.mp4\n",
      "input/Mod05_WrapUp_ver2.mp4\n",
      "input/Mod06_Intro.mp4\n",
      "input/Mod06_Sect01.mp4\n",
      "input/Mod06_Sect02.mp4\n",
      "input/Mod06_WrapUp.mp4\n",
      "input/Mod07_Sect01.mp4\n"
     ]
    }
   ],
   "source": [
    "from boto3 import client\n",
    "\n",
    "conn = client('s3') \n",
    "for key in conn.list_objects(Bucket=bucket)['Contents']:\n",
    "    print(key['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/seaborn/_statistics.py:32: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.22.4)\n",
      "  from scipy.stats import gaussian_kde\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os, io, struct, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "from time import sleep\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_client = boto3.client(\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod01_Course Overview.mp4 transcribed to transcribed-input/Mod01_Course_Overview.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Intro.mp4 transcribed to transcribed-input/Mod02_Intro.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Sect01.mp4 transcribed to transcribed-input/Mod02_Sect01.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Sect02.mp4 transcribed to transcribed-input/Mod02_Sect02.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Sect03.mp4 transcribed to transcribed-input/Mod02_Sect03.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Sect04.mp4 transcribed to transcribed-input/Mod02_Sect04.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_Sect05.mp4 transcribed to transcribed-input/Mod02_Sect05.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod02_WrapUp.mp4 transcribed to transcribed-input/Mod02_WrapUp.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Intro.mp4 transcribed to transcribed-input/Mod03_Intro.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect01.mp4 transcribed to transcribed-input/Mod03_Sect01.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect02_part1.mp4 transcribed to transcribed-input/Mod03_Sect02_part1.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect02_part2.mp4 transcribed to transcribed-input/Mod03_Sect02_part2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect02_part3.mp4 transcribed to transcribed-input/Mod03_Sect02_part3.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect03_part1.mp4 transcribed to transcribed-input/Mod03_Sect03_part1.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect03_part2.mp4 transcribed to transcribed-input/Mod03_Sect03_part2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect03_part3.mp4 transcribed to transcribed-input/Mod03_Sect03_part3.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect04_part1.mp4 transcribed to transcribed-input/Mod03_Sect04_part1.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect04_part2.mp4 transcribed to transcribed-input/Mod03_Sect04_part2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect04_part3.mp4 transcribed to transcribed-input/Mod03_Sect04_part3.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect05.mp4 transcribed to transcribed-input/Mod03_Sect05.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect06.mp4 transcribed to transcribed-input/Mod03_Sect06.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect07_part1.mp4 transcribed to transcribed-input/Mod03_Sect07_part1.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect07_part2.mp4 transcribed to transcribed-input/Mod03_Sect07_part2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect07_part3.mp4 transcribed to transcribed-input/Mod03_Sect07_part3.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_Sect08.mp4 transcribed to transcribed-input/Mod03_Sect08.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod03_WrapUp.mp4 transcribed to transcribed-input/Mod03_WrapUp.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_Intro.mp4 transcribed to transcribed-input/Mod04_Intro.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_Sect01.mp4 transcribed to transcribed-input/Mod04_Sect01.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_Sect02_part1.mp4 transcribed to transcribed-input/Mod04_Sect02_part1.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_Sect02_part2.mp4 transcribed to transcribed-input/Mod04_Sect02_part2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_Sect02_part3.mp4 transcribed to transcribed-input/Mod04_Sect02_part3.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod04_WrapUp.mp4 transcribed to transcribed-input/Mod04_WrapUp.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Intro.mp4 transcribed to transcribed-input/Mod05_Intro.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect01_ver2.mp4 transcribed to transcribed-input/Mod05_Sect01_ver2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect02_part1_ver2.mp4 transcribed to transcribed-input/Mod05_Sect02_part1_ver2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect02_part2.mp4 transcribed to transcribed-input/Mod05_Sect02_part2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect03_part1.mp4 transcribed to transcribed-input/Mod05_Sect03_part1.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect03_part2.mp4 transcribed to transcribed-input/Mod05_Sect03_part2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect03_part3.mp4 transcribed to transcribed-input/Mod05_Sect03_part3.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_Sect03_part4_ver2.mp4 transcribed to transcribed-input/Mod05_Sect03_part4_ver2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod05_WrapUp_ver2.mp4 transcribed to transcribed-input/Mod05_WrapUp_ver2.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod06_Intro.mp4 transcribed to transcribed-input/Mod06_Intro.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod06_Sect01.mp4 transcribed to transcribed-input/Mod06_Sect01.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod06_Sect02.mp4 transcribed to transcribed-input/Mod06_Sect02.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod06_WrapUp.mp4 transcribed to transcribed-input/Mod06_WrapUp.txt\n",
      "s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/input/Mod07_Sect01.mp4 transcribed to transcribed-input/Mod07_Sect01.txt\n"
     ]
    }
   ],
   "source": [
    "output_files=[]\n",
    "transcribe_output_prefix = 'transcribed'\n",
    "for key in conn.list_objects_v2(Bucket=bucket, Prefix='input')['Contents']:\n",
    "    if 'temp' in key['Key']:\n",
    "        continue\n",
    "    object_name=key['Key']\n",
    "    media_input_uri = f's3://{bucket}/{object_name}'\n",
    "\n",
    "    #create the transcription job\n",
    "    job_uuid = uuid.uuid1()\n",
    "    transcribe_job_name = f\"transcribe-job-{job_uuid}\"\n",
    "    output_file = object_name.split('.')[0].replace(\" \",\"_\")\n",
    "    transcribe_output_filename = f'{transcribe_output_prefix}-{output_file}.txt'\n",
    "    output_files.append([transcribe_output_filename,object_name,\"\"])\n",
    "    print(f'{media_input_uri} transcribed to {transcribe_output_filename}')\n",
    "\n",
    "    response = transcribe_client.start_transcription_job(\n",
    "        TranscriptionJobName=transcribe_job_name,\n",
    "        Media={'MediaFileUri': media_input_uri},\n",
    "        MediaFormat='mp4',\n",
    "        LanguageCode='en-US',\n",
    "        OutputBucketName=bucket,\n",
    "        OutputKey=transcribe_output_filename\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['transcribed-input/Mod01_Course_Overview.txt', 'input/Mod01_Course Overview.mp4', ''], ['transcribed-input/Mod02_Intro.txt', 'input/Mod02_Intro.mp4', ''], ['transcribed-input/Mod02_Sect01.txt', 'input/Mod02_Sect01.mp4', ''], ['transcribed-input/Mod02_Sect02.txt', 'input/Mod02_Sect02.mp4', ''], ['transcribed-input/Mod02_Sect03.txt', 'input/Mod02_Sect03.mp4', ''], ['transcribed-input/Mod02_Sect04.txt', 'input/Mod02_Sect04.mp4', ''], ['transcribed-input/Mod02_Sect05.txt', 'input/Mod02_Sect05.mp4', ''], ['transcribed-input/Mod02_WrapUp.txt', 'input/Mod02_WrapUp.mp4', ''], ['transcribed-input/Mod03_Intro.txt', 'input/Mod03_Intro.mp4', ''], ['transcribed-input/Mod03_Sect01.txt', 'input/Mod03_Sect01.mp4', ''], ['transcribed-input/Mod03_Sect02_part1.txt', 'input/Mod03_Sect02_part1.mp4', ''], ['transcribed-input/Mod03_Sect02_part2.txt', 'input/Mod03_Sect02_part2.mp4', ''], ['transcribed-input/Mod03_Sect02_part3.txt', 'input/Mod03_Sect02_part3.mp4', ''], ['transcribed-input/Mod03_Sect03_part1.txt', 'input/Mod03_Sect03_part1.mp4', ''], ['transcribed-input/Mod03_Sect03_part2.txt', 'input/Mod03_Sect03_part2.mp4', ''], ['transcribed-input/Mod03_Sect03_part3.txt', 'input/Mod03_Sect03_part3.mp4', ''], ['transcribed-input/Mod03_Sect04_part1.txt', 'input/Mod03_Sect04_part1.mp4', ''], ['transcribed-input/Mod03_Sect04_part2.txt', 'input/Mod03_Sect04_part2.mp4', ''], ['transcribed-input/Mod03_Sect04_part3.txt', 'input/Mod03_Sect04_part3.mp4', ''], ['transcribed-input/Mod03_Sect05.txt', 'input/Mod03_Sect05.mp4', ''], ['transcribed-input/Mod03_Sect06.txt', 'input/Mod03_Sect06.mp4', ''], ['transcribed-input/Mod03_Sect07_part1.txt', 'input/Mod03_Sect07_part1.mp4', ''], ['transcribed-input/Mod03_Sect07_part2.txt', 'input/Mod03_Sect07_part2.mp4', ''], ['transcribed-input/Mod03_Sect07_part3.txt', 'input/Mod03_Sect07_part3.mp4', ''], ['transcribed-input/Mod03_Sect08.txt', 'input/Mod03_Sect08.mp4', ''], ['transcribed-input/Mod03_WrapUp.txt', 'input/Mod03_WrapUp.mp4', ''], ['transcribed-input/Mod04_Intro.txt', 'input/Mod04_Intro.mp4', ''], ['transcribed-input/Mod04_Sect01.txt', 'input/Mod04_Sect01.mp4', ''], ['transcribed-input/Mod04_Sect02_part1.txt', 'input/Mod04_Sect02_part1.mp4', ''], ['transcribed-input/Mod04_Sect02_part2.txt', 'input/Mod04_Sect02_part2.mp4', ''], ['transcribed-input/Mod04_Sect02_part3.txt', 'input/Mod04_Sect02_part3.mp4', ''], ['transcribed-input/Mod04_WrapUp.txt', 'input/Mod04_WrapUp.mp4', ''], ['transcribed-input/Mod05_Intro.txt', 'input/Mod05_Intro.mp4', ''], ['transcribed-input/Mod05_Sect01_ver2.txt', 'input/Mod05_Sect01_ver2.mp4', ''], ['transcribed-input/Mod05_Sect02_part1_ver2.txt', 'input/Mod05_Sect02_part1_ver2.mp4', ''], ['transcribed-input/Mod05_Sect02_part2.txt', 'input/Mod05_Sect02_part2.mp4', ''], ['transcribed-input/Mod05_Sect03_part1.txt', 'input/Mod05_Sect03_part1.mp4', ''], ['transcribed-input/Mod05_Sect03_part2.txt', 'input/Mod05_Sect03_part2.mp4', ''], ['transcribed-input/Mod05_Sect03_part3.txt', 'input/Mod05_Sect03_part3.mp4', ''], ['transcribed-input/Mod05_Sect03_part4_ver2.txt', 'input/Mod05_Sect03_part4_ver2.mp4', ''], ['transcribed-input/Mod05_WrapUp_ver2.txt', 'input/Mod05_WrapUp_ver2.mp4', ''], ['transcribed-input/Mod06_Intro.txt', 'input/Mod06_Intro.mp4', ''], ['transcribed-input/Mod06_Sect01.txt', 'input/Mod06_Sect01.mp4', ''], ['transcribed-input/Mod06_Sect02.txt', 'input/Mod06_Sect02.mp4', ''], ['transcribed-input/Mod06_WrapUp.txt', 'input/Mod06_WrapUp.mp4', ''], ['transcribed-input/Mod07_Sect01.txt', 'input/Mod07_Sect01.mp4', '']]\n"
     ]
    }
   ],
   "source": [
    "print(output_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETED\n"
     ]
    }
   ],
   "source": [
    "job=None\n",
    "while True:\n",
    "    job = transcribe_client.get_transcription_job(TranscriptionJobName = transcribe_job_name)\n",
    "    if job['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED','FAILED']:\n",
    "        break\n",
    "    print('.', end='')\n",
    "    sleep(20)\n",
    "        \n",
    "print(job['TranscriptionJob']['TranscriptionJobStatus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['transcribed-input/Mod01_Course_Overview.txt', 'input/Mod01_Course Overview.mp4', \"Hi and welcome to Amazon Academy and Machine Learning Foundations in this module, you'll learn about the course objectives, various job roles in the machine learning domain and where you can go to learn more about machine learning. After completing this module, you should be able to identify course prerequisites and objectives indicate the role of the data scientist in business and identify resources for further learning. We're now going to look at the prerequisites for taking this course. Before you take this course, we recommend that you first complete Aws Academy cloud foundations. You should also have some general technical knowledge of it including foundational computer literacy skills like basic computer concepts, email file management and a good understanding of the internet. We also recommend that you have intermediate skills with Python programming and a general knowledge of applied statistics. Finally, general business knowledge is important for this course. This includes insight into how information technology is used in business. It's also important to have business related skill sets such as communication skills, leadership skills, and an orientation towards customer service. In this course, you'll be introduced to the key concepts of machine learning, its tools and its uses you'll also be introduced to and work with some of the AWS services for machine learning. You'll learn how to recognize how machine learning and deep learning are part of artificial intelligence. Describe artificial intelligence and machine learning terminology. Identify how machine learning can be used to solve a business problem. Describe the machine learning process. List the tools available to data scientists and identify when to use machine learning instead of traditional software development methods. As part of this course, you'll also learn how to implement a machine learning pipeline. This includes how to formulate a problem from a business request, obtain and secure data for machine learning, build a Jupiter notebook by using Amazon Sage maker outline the process for evaluating data. Explain why data needs to be preprocess and use open source tools to examine and preprocess data. You will also use Amazon Sage Maker to train and host a machine learning model, use cross validation to test the performance of a machine learning model. Use a hosted model for inference, create an Amazon Sage maker hyper parameter tuning job to optimize a model's effectiveness. And finally how to use managed Amazon machine learning services to solve specific machine learning problems in forecasting computer vision and natural language processing. We'll now review the course outline to achieve the course objectives. You'll complete the following modules to start in module two, you'll get an introduction to machine learning in module three. You'll learn how to implement a machine learning pipeline with Amazon Sage maker modules 45 and six describe how to apply managed Amazon machine learning services for problems in forecasting computer vision and natural language processing. Finally, module seven is a summary of the course. It also includes an overview of steps you can take to work towards the AWS certified machine learning specialty. The next five slides provide more detail about the subtopics covered in each module. The purpose of module two is to introduce you to major concepts for understanding machine learning. Section one describes the overall field of machine learning and how machine learning relates to artificial intelligence and deep learning. In section two, you'll learn about some of the most common business problems you can solve with machine learning. Section three describes the general workflow for solving machine learning problems. You'll also learn some of the more common machine learning terms. In section four, you'll review some of the commonly used tools by machine learning professionals. And lastly in section five, you'll get an overview of some of the common challenges you'll face when working with machine learning problems. In module three, you'll get an introduction to Amazon Sage Maker and how you can use it to implement a machine learning pipeline. The module focuses on the application of machine learning to solve problems with several public domain data sets. As examples of the machine learning pipeline. Section one introduces you to defining business problems and the data sets we will use during this module, section two through eight, describe the phases of the machine learning pipeline by using computer vision as an example application. In section two, you'll learn how to collect and secure data. Section three describes different techniques for evaluating data. In section four, you'll learn about the process of feature engineering. Section five described the steps he'll take to train a model with stage maker. In section six, you'll get an overview of the options in Sage maker for hosting and using a model. Finally, section seven and eight cover how to evaluate and tune your model with Sage Maker. In this module, you'll be introduced to using machine learning to create forecasts based on a time series data. In section one, you'll be introduced to forecasting in some of its common applications. Section two outlines some of the pitfalls of using time series data to make forecasts. Finally, in section three, you'll get an overview of how to use Amazon forecast in this module, you'll learn about using machine learning for computer vision. Section one describes the general problems you can solve with computer vision. In section two, you'll learn about the process for analyzing images and videos. And in section three, you'll learn the steps you'll need to take to prepare data sets for computer vision. In this module, you'll be introduced to natural language processing with machine learning. In section one, you'll learn about the general set of problems you can solve with natural language processing section two reviews. Some of the managed Amazon machine learning services you can use to address natural language processing problems. These services include Amazon transcribe, Amazon translate, Amazon Lex Amazon comprehend and Amazon Poli module seven is the final module of the course. In this module, you'll review what you've learned throughout this course. You'll also be introduced to the next steps you should take. If you want to achieve the Aws Certified Machine learning specialty section, one of this module summarizes the topics you've covered in this course. In section two, you'll learn more about the Aws documentation. You'll also review two common frameworks for applying Aws services. And finally, section three describes the steps you should take. If you want to continue working towards the Aws Certified machine learning specialty in this section, you'll learn about some of the more common job roles for machine learning professionals. If you're interested in a data scientist role, focus on developing analytical statistical and programming skills. As a data scientist, you'll use those skills to collect analyze and interpret large data sets. Some universities now offer degrees in data science, but data scientists often have degrees in related fields like statistics, math, computer science or economics. As a data scientist, you'll need technical competencies in statistics, machine learning, programming languages and data analytics. If you'd like to have a career as a machine learning engineer, the skills you'll need will be similar to a data scientist, skill set like data scientists, machine learning engineers also require technical competencies in statistics and machine learning. However, you'll focus more on programming skills and software architecture than analysis and interpretation. As a machine learning engineer, you'll apply those programming and architecture skills to design and develop machine learning systems. Machine learning engineers often have previous experience with software development and they rely more heavily on programming and software engineering than other machine learning roles. You might also be interested in a career in science where you can apply machine learning technology to your field. Machine learning is having an impact in everything from astronomy to zoology. So there are many different paths open to you. As an applied science researcher, your primary focus will be on the type of science you're working on. You'll need some of the same skills as a data scientist, but you'll also need to know how to apply those skills to your chosen domain. Thus, applied science rules also require technical competencies in statistics and machine learning. Many software developers are now integrating machine learning into their applications. If you're interested in a career as a software developer, you should also include machine learning technology in your studies. As a machine learning developer, your primary focus will be software development skills, but you'll also need some of the same skills as a data scientist. So make sure you take coursework in statistics and applied mathematics. And here's a final note for this module, we recommend reviewing your student guide in your student guide. You'll find links to documentation and other resources you'll use throughout the course. That's it for this introduction. Thanks for watching. We'll see you in the next video.\"]\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "transcribed_text = []\n",
    "for transcribe_output_filename in output_files:\n",
    "    result = s3_client.get_object(Bucket=bucket, Key=transcribe_output_filename[0]) \n",
    "    data = json.load(result['Body']) \n",
    "    transcription = data['results']['transcripts'][0]['transcript']\n",
    "    transcribe_output_filename[2] = transcription\n",
    "\n",
    "print(output_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalizing the text\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to perform any text normalization steps that are necessary for your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=output_files, columns=['OutputFile','Video','Transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OutputFile</th>\n",
       "      <th>Video</th>\n",
       "      <th>Transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcribed-input/Mod01_Course_Overview.txt</td>\n",
       "      <td>input/Mod01_Course Overview.mp4</td>\n",
       "      <td>Hi and welcome to Amazon Academy and Machine L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transcribed-input/Mod02_Intro.txt</td>\n",
       "      <td>input/Mod02_Intro.mp4</td>\n",
       "      <td>Hi and welcome to module two of Aws Academy ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transcribed-input/Mod02_Sect01.txt</td>\n",
       "      <td>input/Mod02_Sect01.mp4</td>\n",
       "      <td>Hi and welcome to section one in this section....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transcribed-input/Mod02_Sect02.txt</td>\n",
       "      <td>input/Mod02_Sect02.mp4</td>\n",
       "      <td>Hi and welcome back in this section. We're goi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transcribed-input/Mod02_Sect03.txt</td>\n",
       "      <td>input/Mod02_Sect03.mp4</td>\n",
       "      <td>Hi and welcome back. This is section three and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    OutputFile  \\\n",
       "0  transcribed-input/Mod01_Course_Overview.txt   \n",
       "1            transcribed-input/Mod02_Intro.txt   \n",
       "2           transcribed-input/Mod02_Sect01.txt   \n",
       "3           transcribed-input/Mod02_Sect02.txt   \n",
       "4           transcribed-input/Mod02_Sect03.txt   \n",
       "\n",
       "                             Video  \\\n",
       "0  input/Mod01_Course Overview.mp4   \n",
       "1            input/Mod02_Intro.mp4   \n",
       "2           input/Mod02_Sect01.mp4   \n",
       "3           input/Mod02_Sect02.mp4   \n",
       "4           input/Mod02_Sect03.mp4   \n",
       "\n",
       "                                       Transcription  \n",
       "0  Hi and welcome to Amazon Academy and Machine L...  \n",
       "1  Hi and welcome to module two of Aws Academy ma...  \n",
       "2  Hi and welcome to section one in this section....  \n",
       "3  Hi and welcome back in this section. We're goi...  \n",
       "4  Hi and welcome back. This is section three and...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(content):\n",
    "    text = re.sub(r\"http\\S+\", \"\", content ) # Remove urls\n",
    "    text = text.lower() # Lowercase \n",
    "    text = text.strip() # Remove leading/trailing whitespace\n",
    "    text = re.sub('\\s+', ' ', text) # Remove extra space and tabs\n",
    "    text = re.sub('\\n',' ',text) # remove newlines\n",
    "    text = re.compile('<.*?>').sub('', text) # Remove HTML tags/markups:\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 ms, sys: 0 ns, total: 12.7 ms\n",
      "Wall time: 12.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['Transcription_normalized'] = df['Transcription'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OutputFile</th>\n",
       "      <th>Video</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>Transcription_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcribed-input/Mod01_Course_Overview.txt</td>\n",
       "      <td>input/Mod01_Course Overview.mp4</td>\n",
       "      <td>Hi and welcome to Amazon Academy and Machine Learning Foundations in this module, you'll learn about the course objectives, various job roles in t...</td>\n",
       "      <td>hi and welcome to amazon academy and machine learning foundations in this module, you'll learn about the course objectives, various job roles in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transcribed-input/Mod02_Intro.txt</td>\n",
       "      <td>input/Mod02_Intro.mp4</td>\n",
       "      <td>Hi and welcome to module two of Aws Academy machine learning in this module, we're going to introduce machine learning. We'll first look at the bu...</td>\n",
       "      <td>hi and welcome to module two of aws academy machine learning in this module, we're going to introduce machine learning. we'll first look at the bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transcribed-input/Mod02_Sect01.txt</td>\n",
       "      <td>input/Mod02_Sect01.mp4</td>\n",
       "      <td>Hi and welcome to section one in this section. We're going to talk about what machine learning is. This course is an introduction to machine learn...</td>\n",
       "      <td>hi and welcome to section one in this section. we're going to talk about what machine learning is. this course is an introduction to machine learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transcribed-input/Mod02_Sect02.txt</td>\n",
       "      <td>input/Mod02_Sect02.mp4</td>\n",
       "      <td>Hi and welcome back in this section. We're going to look at the types of business problems. Machine learning can help you solve. Machine learning ...</td>\n",
       "      <td>hi and welcome back in this section. we're going to look at the types of business problems. machine learning can help you solve. machine learning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transcribed-input/Mod02_Sect03.txt</td>\n",
       "      <td>input/Mod02_Sect03.mp4</td>\n",
       "      <td>Hi and welcome back. This is section three and we're going to give you a quick high level overview of machine learning terminology and a typical w...</td>\n",
       "      <td>hi and welcome back. this is section three and we're going to give you a quick high level overview of machine learning terminology and a typical w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    OutputFile  \\\n",
       "0  transcribed-input/Mod01_Course_Overview.txt   \n",
       "1            transcribed-input/Mod02_Intro.txt   \n",
       "2           transcribed-input/Mod02_Sect01.txt   \n",
       "3           transcribed-input/Mod02_Sect02.txt   \n",
       "4           transcribed-input/Mod02_Sect03.txt   \n",
       "\n",
       "                             Video  \\\n",
       "0  input/Mod01_Course Overview.mp4   \n",
       "1            input/Mod02_Intro.mp4   \n",
       "2           input/Mod02_Sect01.mp4   \n",
       "3           input/Mod02_Sect02.mp4   \n",
       "4           input/Mod02_Sect03.mp4   \n",
       "\n",
       "                                                                                                                                           Transcription  \\\n",
       "0  Hi and welcome to Amazon Academy and Machine Learning Foundations in this module, you'll learn about the course objectives, various job roles in t...   \n",
       "1  Hi and welcome to module two of Aws Academy machine learning in this module, we're going to introduce machine learning. We'll first look at the bu...   \n",
       "2  Hi and welcome to section one in this section. We're going to talk about what machine learning is. This course is an introduction to machine learn...   \n",
       "3  Hi and welcome back in this section. We're going to look at the types of business problems. Machine learning can help you solve. Machine learning ...   \n",
       "4  Hi and welcome back. This is section three and we're going to give you a quick high level overview of machine learning terminology and a typical w...   \n",
       "\n",
       "                                                                                                                                Transcription_normalized  \n",
       "0  hi and welcome to amazon academy and machine learning foundations in this module, you'll learn about the course objectives, various job roles in t...  \n",
       "1  hi and welcome to module two of aws academy machine learning in this module, we're going to introduce machine learning. we'll first look at the bu...  \n",
       "2  hi and welcome to section one in this section. we're going to talk about what machine learning is. this course is an introduction to machine learn...  \n",
       "3  hi and welcome back in this section. we're going to look at the types of business problems. machine learning can help you solve. machine learning ...  \n",
       "4  hi and welcome back. this is section three and we're going to give you a quick high level overview of machine learning terminology and a typical w...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 150)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting key phrases and topics\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to extract the key phrases and topics from the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.Session().resource('s3')\n",
    "\n",
    "def upload_comprehend_s3_csv(filename, folder, dataframe):\n",
    "    csv_buffer = io.StringIO()\n",
    "    \n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False )\n",
    "    s3_resource.Bucket(bucket).Object(os.path.join(prefix, folder, filename)).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded input to s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/capstone/comprehend/comprehend_input.csv\n"
     ]
    }
   ],
   "source": [
    "comprehend_file = 'comprehend_input.csv'\n",
    "prefix='capstone'\n",
    "upload_comprehend_s3_csv(comprehend_file, 'comprehend', df['Transcription_normalized'].str.slice(0,5000))\n",
    "test_url = f's3://{bucket}/{prefix}/comprehend/{comprehend_file}'\n",
    "print(f'Uploaded input to {test_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehend client information\n",
    "comprehend_client = boto3.client(service_name=\"comprehend\")\n",
    "\n",
    "# Other job parameters\n",
    "input_data_format = 'ONE_DOC_PER_LINE'\n",
    "job_uuid = uuid.uuid1()\n",
    "job_name = f\"kpe-job-{job_uuid}\"\n",
    "input_data_s3_path = test_url\n",
    "output_data_s3_path = f's3://{bucket}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the inference job\n",
    "kpe_response = comprehend_client.start_key_phrases_detection_job(\n",
    "    InputDataConfig={'S3Uri': input_data_s3_path,\n",
    "                     'InputFormat': input_data_format},\n",
    "    OutputDataConfig={'S3Uri': output_data_s3_path},\n",
    "    DataAccessRoleArn=job_data_access_role,\n",
    "    JobName=job_name,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "# Get the job ID\n",
    "kpe_job_id = kpe_response['JobId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'entity-job-{job_uuid}'\n",
    "entity_response = comprehend_client.start_entities_detection_job(\n",
    "    InputDataConfig={'S3Uri': input_data_s3_path,\n",
    "                     'InputFormat': input_data_format},\n",
    "    OutputDataConfig={'S3Uri': output_data_s3_path},\n",
    "    DataAccessRoleArn=job_data_access_role,\n",
    "    JobName=job_name,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "# Get the job ID\n",
    "entity_job_id = entity_response['JobId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating the dashboard\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to create the dashboard for your solution.\n",
    "\n",
    "Use the link below to obtain the IP address of your computer.\n",
    "\n",
    "http://checkip.amazonaws.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ip = \"41.45.91.210/24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (23.3.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.2\n",
      "    Uninstalling pip-23.3.2:\n",
      "      Successfully uninstalled pip-23.3.2\n",
      "Successfully installed pip-24.2\n",
      "Collecting opensearch\n",
      "  Downloading opensearch-0.9.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[29 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Warning: 'classifiers' should be a list, got type 'filter'\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-yypu3wj3/opensearch_81485268611a4b9b971370f1c68ed662/setup.py\", line 14, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 184, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 200, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 970, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/dist.py\", line 956, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 989, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 310, in run\n",
      "  \u001b[31m   \u001b[0m     self.find_sources()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 318, in find_sources\n",
      "  \u001b[31m   \u001b[0m     mm.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 544, in run\n",
      "  \u001b[31m   \u001b[0m     self.prune_file_list()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/command/egg_info.py\", line 610, in prune_file_list\n",
      "  \u001b[31m   \u001b[0m     base_dir = self.distribution.get_fullname()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_core_metadata.py\", line 266, in get_fullname\n",
      "  \u001b[31m   \u001b[0m     return _distribution_fullname(self.get_name(), self.get_version())\n",
      "  \u001b[31m   \u001b[0m   File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/setuptools/_core_metadata.py\", line 284, in _distribution_fullname\n",
      "  \u001b[31m   \u001b[0m     canonicalize_version(version, strip_trailing_zero=False),\n",
      "  \u001b[31m   \u001b[0m TypeError: canonicalize_version() got an unexpected keyword argument 'strip_trailing_zero'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hCollecting opensearch-py\n",
      "  Downloading opensearch_py-2.7.1-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py) (2.9.0)\n",
      "Requirement already satisfied: certifi>=2024.07.04 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py) (2024.7.4)\n",
      "Collecting Events (from opensearch-py)\n",
      "  Downloading Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: urllib3!=2.2.0,!=2.2.1,<3,>=1.26.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py) (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.0->opensearch-py) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.0->opensearch-py) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil->opensearch-py) (1.16.0)\n",
      "Downloading opensearch_py-2.7.1-py3-none-any.whl (325 kB)\n",
      "Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Installing collected packages: Events, opensearch-py\n",
      "Successfully installed Events-0.5 opensearch-py-2.7.1\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (2024.7.4)\n",
      "Collecting requests-aws4auth\n",
      "  Downloading requests_aws4auth-1.3.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests-aws4auth) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->requests-aws4auth) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->requests-aws4auth) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->requests-aws4auth) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->requests-aws4auth) (2024.7.4)\n",
      "Downloading requests_aws4auth-1.3.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: requests-aws4auth\n",
      "Successfully installed requests-aws4auth-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install opensearch\n",
    "!pip install opensearch-py\n",
    "!pip install requests\n",
    "!pip install requests-aws4auth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an boto3 client for OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = boto3.client('es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following sets up an access policy so that only your ip address can access the OpenSearch dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"AWS\": \"*\"\n",
    "                },\n",
    "                \"Action\": \"es:*\",\n",
    "                \"Resource\": \"*\",\n",
    "                \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": my_ip } }\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the OpenSearch cluster using the following options:\n",
    "\n",
    "1) DomainName - is the name of the OpenSearch cluster\n",
    "2) ElasticSearchClusterConfig - specifies the instance type, the number of instances, whether a dedicated master is required, and if the cluster should be multi-zoned\n",
    "3) AccessPolicies - contains the statement from above that restricts access to only your IP address\n",
    "4) ElasticsearchVersion - using 7.9 version as this is known to work with the rest of the notebook. You can try other versions, but we recommend 7.9 to complete this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = es_client.create_elasticsearch_domain(\n",
    "    DomainName = 'nlp-lab',\n",
    "    ElasticsearchVersion = '7.9',\n",
    "    ElasticsearchClusterConfig={\n",
    "        \"InstanceType\": 'm3.large.elasticsearch',\n",
    "        \"InstanceCount\": 2,\n",
    "        \"DedicatedMasterEnabled\": False,\n",
    "        \"ZoneAwarenessEnabled\": False\n",
    "    },\n",
    "    AccessPolicies = json.dumps(access_policy)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenSearch typically takes around 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................Ready\n"
     ]
    }
   ],
   "source": [
    "# Get current job status\n",
    "kpe_job = comprehend_client.describe_key_phrases_detection_job(JobId=kpe_job_id)\n",
    "\n",
    "# Loop until job is completed\n",
    "waited = 0\n",
    "timeout_minutes = 30\n",
    "while kpe_job['KeyPhrasesDetectionJobProperties']['JobStatus'] != 'COMPLETED':\n",
    "    sleep(10)\n",
    "    waited += 10\n",
    "    assert waited//60 < timeout_minutes, \"Job timed out after %d seconds.\" % waited\n",
    "    print('.', end='')\n",
    "    kpe_job = comprehend_client.describe_key_phrases_detection_job(JobId=kpe_job_id)\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............Ready\n"
     ]
    }
   ],
   "source": [
    "# Get current job status\n",
    "entity_job = comprehend_client.describe_entities_detection_job(JobId=entity_job_id)\n",
    "\n",
    "# Loop until job is completed\n",
    "waited = 0\n",
    "timeout_minutes = 30\n",
    "while entity_job['EntitiesDetectionJobProperties']['JobStatus'] != 'COMPLETED':\n",
    "    sleep(10)\n",
    "    waited += 10\n",
    "    assert waited//60 < timeout_minutes, \"Job timed out after %d seconds.\" % waited\n",
    "    print('.', end='')\n",
    "    entity_job = comprehend_client.describe_entities_detection_job(JobId=entity_job_id)\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the results for both cells say 'Ready' you can proceed.\n",
    "\n",
    "Get the output for the Key Phrases detection Job by extracting the output location from the job and downloading it to the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output filename: s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/592996473068-KP-79a0d393c129ad614a7c36912e1b714e/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "kpe_comprehend_output_file = kpe_job['KeyPhrasesDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "print(f'output filename: {kpe_comprehend_output_file}')\n",
    "\n",
    "kpe_comprehend_bucket, kpe_comprehend_key = kpe_comprehend_output_file.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "\n",
    "s3r = boto3.resource('s3')\n",
    "s3r.meta.client.download_file(kpe_comprehend_bucket, kpe_comprehend_key, 'output-kpe.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, extract the file and rename the output so we know which file this is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the tar file\n",
    "import tarfile\n",
    "tf = tarfile.open('output-kpe.tar.gz')\n",
    "tf.extractall()\n",
    "# Rename the output\n",
    "!mv 'output' 'kpe_output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat the above process for the entity detection job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output filename: s3://c127808a3228856l7912640t1w592996473068-labbucket-290g8ee8olew/592996473068-NER-51575ff34c7a3af1a562625590b06a1c/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "entity_comprehend_output_file = entity_job['EntitiesDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "print(f'output filename: {entity_comprehend_output_file}')\n",
    "\n",
    "entity_comprehend_bucket, entity_comprehend_key = entity_comprehend_output_file.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "\n",
    "s3r = boto3.resource('s3')\n",
    "s3r.meta.client.download_file(entity_comprehend_bucket, entity_comprehend_key, 'output-entity.tar.gz')\n",
    "\n",
    "# Extract the tar file\n",
    "import tarfile\n",
    "tf = tarfile.open('output-entity.tar.gz')\n",
    "tf.extractall()\n",
    "# Rename the output\n",
    "!mv 'output' 'entity_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the Key Phrases file into an array.\n",
    "import json\n",
    "data = []\n",
    "with open ('kpe_output', \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KeyPhrases</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'BeginOffset': 26, 'EndOffset': 54, 'Score': 0.832979081503515, 'Text': 'academy and machine learning'}, {'BeginOffset': 55, 'EndOffset': 66, 'S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.6214939014751475, 'Text': 'module two'}, {'BeginOffset': 33, 'EndOffset': 52, 'Score': 0.99413129...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.9576599228626844, 'Text': 'section three'}, {'BeginOffset': 72, 'EndOffset': 99, 'Score': 0.99875...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'BeginOffset': 17, 'EndOffset': 29, 'Score': 0.9997639133384972, 'Text': 'this section'}, {'BeginOffset': 53, 'EndOffset': 62, 'Score': 0.999938...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.9879459585199664, 'Text': 'section one'}, {'BeginOffset': 34, 'EndOffset': 46, 'Score': 0.9997940...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              KeyPhrases  \\\n",
       "0  [{'BeginOffset': 26, 'EndOffset': 54, 'Score': 0.832979081503515, 'Text': 'academy and machine learning'}, {'BeginOffset': 55, 'EndOffset': 66, 'S...   \n",
       "1  [{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.6214939014751475, 'Text': 'module two'}, {'BeginOffset': 33, 'EndOffset': 52, 'Score': 0.99413129...   \n",
       "2  [{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.9576599228626844, 'Text': 'section three'}, {'BeginOffset': 72, 'EndOffset': 99, 'Score': 0.99875...   \n",
       "3  [{'BeginOffset': 17, 'EndOffset': 29, 'Score': 0.9997639133384972, 'Text': 'this section'}, {'BeginOffset': 53, 'EndOffset': 62, 'Score': 0.999938...   \n",
       "4  [{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.9879459585199664, 'Text': 'section one'}, {'BeginOffset': 34, 'EndOffset': 46, 'Score': 0.9997940...   \n",
       "\n",
       "   Line  \n",
       "0     0  \n",
       "1     1  \n",
       "2     4  \n",
       "3     5  \n",
       "4     2  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data array into a dataframe. There are two columns, KeyPhrases and Line.\n",
    "kpdf = pd.DataFrame(data, columns=['KeyPhrases','Line'])\n",
    "kpdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can repeat the last 2 steps for the entities data.\n",
    "import json\n",
    "data = []\n",
    "with open ('entity_output', \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 547, 'EndOff...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.8810036668646961, 'Text': 'module two', 'Type': 'OTHER'}, {'BeginOffset': 33, 'EndOffset': 36, 'S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.6284302444453367, 'Text': 'section three', 'Type': 'OTHER'}, {'BeginOffset': 460, 'EndOffset': 46...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'BeginOffset': 169, 'EndOffset': 182, 'Score': 0.5795286755807814, 'Text': 'all the tools', 'Type': 'QUANTITY'}, {'BeginOffset': 193, 'EndOffset...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.8874841148902434, 'Text': 'section one', 'Type': 'OTHER'}, {'BeginOffset': 183, 'EndOffset': 188,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                Entities  \\\n",
       "0  [{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 547, 'EndOff...   \n",
       "1  [{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.8810036668646961, 'Text': 'module two', 'Type': 'OTHER'}, {'BeginOffset': 33, 'EndOffset': 36, 'S...   \n",
       "2  [{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.6284302444453367, 'Text': 'section three', 'Type': 'OTHER'}, {'BeginOffset': 460, 'EndOffset': 46...   \n",
       "3  [{'BeginOffset': 169, 'EndOffset': 182, 'Score': 0.5795286755807814, 'Text': 'all the tools', 'Type': 'QUANTITY'}, {'BeginOffset': 193, 'EndOffset...   \n",
       "4  [{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.8874841148902434, 'Text': 'section one', 'Type': 'OTHER'}, {'BeginOffset': 183, 'EndOffset': 188,...   \n",
       "\n",
       "   Line  \n",
       "0     0  \n",
       "1     1  \n",
       "2     4  \n",
       "3     5  \n",
       "4     2  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entitydf = pd.DataFrame(data, columns=['Entities','Line'])\n",
    "entitydf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the entities. the different detected entities are burried in the same fields. Depending on your scenario, you may want to split this out into separate columns for each entity type. To do this we can write a function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(entities, entity_type):\n",
    "    filtered_entities=[]\n",
    "    for entity in entities:\n",
    "        if entity['Type'] == entity_type:\n",
    "            filtered_entities.append(entity)\n",
    "    return filtered_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Line</th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 547, 'EndOff...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 562, 'EndOff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.8810036668646961, 'Text': 'module two', 'Type': 'OTHER'}, {'BeginOffset': 33, 'EndOffset': 36, 'S...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': 'ORGANIZATION'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.6284302444453367, 'Text': 'section three', 'Type': 'OTHER'}, {'BeginOffset': 460, 'EndOffset': 46...</td>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'BeginOffset': 169, 'EndOffset': 182, 'Score': 0.5795286755807814, 'Text': 'all the tools', 'Type': 'QUANTITY'}, {'BeginOffset': 193, 'EndOffset...</td>\n",
       "      <td>5</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 305, 'EndOffset': 312, 'Score': 0.4635194962558986, 'Text': 'jupiter', 'Type': 'ORGANIZATION'}, {'BeginOffset': 699, 'EndOffset':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.8874841148902434, 'Text': 'section one', 'Type': 'OTHER'}, {'BeginOffset': 183, 'EndOffset': 188,...</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                Entities  \\\n",
       "0  [{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 547, 'EndOff...   \n",
       "1  [{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.8810036668646961, 'Text': 'module two', 'Type': 'OTHER'}, {'BeginOffset': 33, 'EndOffset': 36, 'S...   \n",
       "2  [{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.6284302444453367, 'Text': 'section three', 'Type': 'OTHER'}, {'BeginOffset': 460, 'EndOffset': 46...   \n",
       "3  [{'BeginOffset': 169, 'EndOffset': 182, 'Score': 0.5795286755807814, 'Text': 'all the tools', 'Type': 'QUANTITY'}, {'BeginOffset': 193, 'EndOffset...   \n",
       "4  [{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.8874841148902434, 'Text': 'section one', 'Type': 'OTHER'}, {'BeginOffset': 183, 'EndOffset': 188,...   \n",
       "\n",
       "   Line location  \\\n",
       "0     0       []   \n",
       "1     1       []   \n",
       "2     4       []   \n",
       "3     5       []   \n",
       "4     2       []   \n",
       "\n",
       "                                                                                                                                            organization  \n",
       "0  [{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 562, 'EndOff...  \n",
       "1                                             [{'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': 'ORGANIZATION'}]  \n",
       "2                                                                                                                                                     []  \n",
       "3  [{'BeginOffset': 305, 'EndOffset': 312, 'Score': 0.4635194962558986, 'Text': 'jupiter', 'Type': 'ORGANIZATION'}, {'BeginOffset': 699, 'EndOffset':...  \n",
       "4                                                                                                                                                     []  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we can apply the function to each of the event types we want to extract.\n",
    "# df['plot_normalized'] = df['plot'].apply(normalize_text)    \n",
    "entitydf['location'] = entitydf['Entities'].apply(lambda x: extract_entities(x, 'LOCATION'))\n",
    "entitydf['organization'] = entitydf['Entities'].apply(lambda x: extract_entities(x, 'ORGANIZATION'))\n",
    "\n",
    "entitydf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the results from Comprehend loaded into dataframes, it's time to merge everything together. The Line will merge together the results from Comprehend with the original dataframe.\n",
    "\n",
    "Start by setting the index on both results dataframes to the Line column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 547, 'EndOff...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 562, 'EndOff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.8810036668646961, 'Text': 'module two', 'Type': 'OTHER'}, {'BeginOffset': 33, 'EndOffset': 36, 'S...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': 'ORGANIZATION'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.8874841148902434, 'Text': 'section one', 'Type': 'OTHER'}, {'BeginOffset': 183, 'EndOffset': 188,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'BeginOffset': 763, 'EndOffset': 767, 'Score': 0.5536403180912558, 'Text': 'more', 'Type': 'QUANTITY'}, {'BeginOffset': 935, 'EndOffset': 951, '...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.6284302444453367, 'Text': 'section three', 'Type': 'OTHER'}, {'BeginOffset': 460, 'EndOffset': 46...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                   Entities  \\\n",
       "Line                                                                                                                                                          \n",
       "0     [{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 547, 'EndOff...   \n",
       "1     [{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.8810036668646961, 'Text': 'module two', 'Type': 'OTHER'}, {'BeginOffset': 33, 'EndOffset': 36, 'S...   \n",
       "2     [{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.8874841148902434, 'Text': 'section one', 'Type': 'OTHER'}, {'BeginOffset': 183, 'EndOffset': 188,...   \n",
       "3     [{'BeginOffset': 763, 'EndOffset': 767, 'Score': 0.5536403180912558, 'Text': 'more', 'Type': 'QUANTITY'}, {'BeginOffset': 935, 'EndOffset': 951, '...   \n",
       "4     [{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.6284302444453367, 'Text': 'section three', 'Type': 'OTHER'}, {'BeginOffset': 460, 'EndOffset': 46...   \n",
       "\n",
       "     location  \\\n",
       "Line            \n",
       "0          []   \n",
       "1          []   \n",
       "2          []   \n",
       "3          []   \n",
       "4          []   \n",
       "\n",
       "                                                                                                                                               organization  \n",
       "Line                                                                                                                                                         \n",
       "0     [{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 562, 'EndOff...  \n",
       "1                                                [{'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': 'ORGANIZATION'}]  \n",
       "2                                                                                                                                                        []  \n",
       "3                                                                                                                                                        []  \n",
       "4                                                                                                                                                        []  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entitydf.set_index('Line', inplace = True)\n",
    "entitydf.sort_index(inplace=True)\n",
    "kpdf.set_index('Line', inplace=True)\n",
    "kpdf.sort_index(inplace=True)\n",
    "entitydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KeyPhrases</th>\n",
       "      <th>Entities</th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Line</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'BeginOffset': 26, 'EndOffset': 54, 'Score': 0.832979081503515, 'Text': 'academy and machine learning'}, {'BeginOffset': 55, 'EndOffset': 66, 'Score': 0.6903008712888459, 'Text': 'foundations'},...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 547, 'EndOffset': 552, 'Score': 0.9491953642972518, 'Text': 'f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 562, 'EndOffset': 573, 'Score': 0.9465551617295611, 'Text': 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.6214939014751475, 'Text': 'module two'}, {'BeginOffset': 33, 'EndOffset': 52, 'Score': 0.9941312955832783, 'Text': 'aws academy machine'}, {'BeginO...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.8810036668646961, 'Text': 'module two', 'Type': 'OTHER'}, {'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': 'ORGANIZATION'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.9879459585199664, 'Text': 'section one'}, {'BeginOffset': 34, 'EndOffset': 46, 'Score': 0.9997940602015712, 'Text': 'this section'}, {'BeginOffset'...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.8874841148902434, 'Text': 'section one', 'Type': 'OTHER'}, {'BeginOffset': 183, 'EndOffset': 188, 'Score': 0.9281728391309813, 'Text': 'first', 'Ty...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'BeginOffset': 24, 'EndOffset': 36, 'Score': 0.9996331597282907, 'Text': 'this section'}, {'BeginOffset': 61, 'EndOffset': 70, 'Score': 0.999921626630556, 'Text': 'the types'}, {'BeginOffset': 7...</td>\n",
       "      <td>[{'BeginOffset': 763, 'EndOffset': 767, 'Score': 0.5536403180912558, 'Text': 'more', 'Type': 'QUANTITY'}, {'BeginOffset': 935, 'EndOffset': 951, 'Score': 0.9698888667844483, 'Text': 'three main ty...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.9576599228626844, 'Text': 'section three'}, {'BeginOffset': 72, 'EndOffset': 99, 'Score': 0.9987502879769787, 'Text': 'a quick high level overview'...</td>\n",
       "      <td>[{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.6284302444453367, 'Text': 'section three', 'Type': 'OTHER'}, {'BeginOffset': 460, 'EndOffset': 468, 'Score': 0.9770625204054991, 'Text': 'one task'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                   KeyPhrases  \\\n",
       "Line                                                                                                                                                                                                            \n",
       "0     [{'BeginOffset': 26, 'EndOffset': 54, 'Score': 0.832979081503515, 'Text': 'academy and machine learning'}, {'BeginOffset': 55, 'EndOffset': 66, 'Score': 0.6903008712888459, 'Text': 'foundations'},...   \n",
       "1     [{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.6214939014751475, 'Text': 'module two'}, {'BeginOffset': 33, 'EndOffset': 52, 'Score': 0.9941312955832783, 'Text': 'aws academy machine'}, {'BeginO...   \n",
       "2     [{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.9879459585199664, 'Text': 'section one'}, {'BeginOffset': 34, 'EndOffset': 46, 'Score': 0.9997940602015712, 'Text': 'this section'}, {'BeginOffset'...   \n",
       "3     [{'BeginOffset': 24, 'EndOffset': 36, 'Score': 0.9996331597282907, 'Text': 'this section'}, {'BeginOffset': 61, 'EndOffset': 70, 'Score': 0.999921626630556, 'Text': 'the types'}, {'BeginOffset': 7...   \n",
       "4     [{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.9576599228626844, 'Text': 'section three'}, {'BeginOffset': 72, 'EndOffset': 99, 'Score': 0.9987502879769787, 'Text': 'a quick high level overview'...   \n",
       "\n",
       "                                                                                                                                                                                                     Entities  \\\n",
       "Line                                                                                                                                                                                                            \n",
       "0     [{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 547, 'EndOffset': 552, 'Score': 0.9491953642972518, 'Text': 'f...   \n",
       "1     [{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.8810036668646961, 'Text': 'module two', 'Type': 'OTHER'}, {'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': ...   \n",
       "2     [{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.8874841148902434, 'Text': 'section one', 'Type': 'OTHER'}, {'BeginOffset': 183, 'EndOffset': 188, 'Score': 0.9281728391309813, 'Text': 'first', 'Ty...   \n",
       "3     [{'BeginOffset': 763, 'EndOffset': 767, 'Score': 0.5536403180912558, 'Text': 'more', 'Type': 'QUANTITY'}, {'BeginOffset': 935, 'EndOffset': 951, 'Score': 0.9698888667844483, 'Text': 'three main ty...   \n",
       "4     [{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.6284302444453367, 'Text': 'section three', 'Type': 'OTHER'}, {'BeginOffset': 460, 'EndOffset': 468, 'Score': 0.9770625204054991, 'Text': 'one task'...   \n",
       "\n",
       "     location  \\\n",
       "Line            \n",
       "0          []   \n",
       "1          []   \n",
       "2          []   \n",
       "3          []   \n",
       "4          []   \n",
       "\n",
       "                                                                                                                                                                                                 organization  \n",
       "Line                                                                                                                                                                                                           \n",
       "0     [{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 562, 'EndOffset': 573, 'Score': 0.9465551617295611, 'Text': 'a...  \n",
       "1                                                                                                  [{'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': 'ORGANIZATION'}]  \n",
       "2                                                                                                                                                                                                          []  \n",
       "3                                                                                                                                                                                                          []  \n",
       "4                                                                                                                                                                                                          []  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, merge the kpdf dataframe with the entitydf dataframe.\n",
    "m1 = kpdf.merge(entitydf, left_index=True, right_index=True)\n",
    "m1.sort_index(inplace=True)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "m1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now merge the m1 dataframe with the original dataframe df.\n",
    "mergedDf = df.merge(m1, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OutputFile</th>\n",
       "      <th>Video</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>Transcription_normalized</th>\n",
       "      <th>KeyPhrases</th>\n",
       "      <th>Entities</th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcribed-input/Mod01_Course_Overview.txt</td>\n",
       "      <td>input/Mod01_Course Overview.mp4</td>\n",
       "      <td>Hi and welcome to Amazon Academy and Machine Learning Foundations in this module, you'll learn about the course objectives, various job roles in the machine learning domain and where you can go to...</td>\n",
       "      <td>hi and welcome to amazon academy and machine learning foundations in this module, you'll learn about the course objectives, various job roles in the machine learning domain and where you can go to...</td>\n",
       "      <td>[{'BeginOffset': 26, 'EndOffset': 54, 'Score': 0.832979081503515, 'Text': 'academy and machine learning'}, {'BeginOffset': 55, 'EndOffset': 66, 'Score': 0.6903008712888459, 'Text': 'foundations'},...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 547, 'EndOffset': 552, 'Score': 0.9491953642972518, 'Text': 'f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 562, 'EndOffset': 573, 'Score': 0.9465551617295611, 'Text': 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transcribed-input/Mod02_Intro.txt</td>\n",
       "      <td>input/Mod02_Intro.mp4</td>\n",
       "      <td>Hi and welcome to module two of Aws Academy machine learning in this module, we're going to introduce machine learning. We'll first look at the business problems that can be solved by machine lear...</td>\n",
       "      <td>hi and welcome to module two of aws academy machine learning in this module, we're going to introduce machine learning. we'll first look at the business problems that can be solved by machine lear...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.6214939014751475, 'Text': 'module two'}, {'BeginOffset': 33, 'EndOffset': 52, 'Score': 0.9941312955832783, 'Text': 'aws academy machine'}, {'BeginO...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.8810036668646961, 'Text': 'module two', 'Type': 'OTHER'}, {'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': 'ORGANIZATION'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transcribed-input/Mod02_Sect01.txt</td>\n",
       "      <td>input/Mod02_Sect01.mp4</td>\n",
       "      <td>Hi and welcome to section one in this section. We're going to talk about what machine learning is. This course is an introduction to machine learning, which is also known as ML. But first we'll di...</td>\n",
       "      <td>hi and welcome to section one in this section. we're going to talk about what machine learning is. this course is an introduction to machine learning, which is also known as ml. but first we'll di...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.9879459585199664, 'Text': 'section one'}, {'BeginOffset': 34, 'EndOffset': 46, 'Score': 0.9997940602015712, 'Text': 'this section'}, {'BeginOffset'...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.8874841148902434, 'Text': 'section one', 'Type': 'OTHER'}, {'BeginOffset': 183, 'EndOffset': 188, 'Score': 0.9281728391309813, 'Text': 'first', 'Ty...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transcribed-input/Mod02_Sect02.txt</td>\n",
       "      <td>input/Mod02_Sect02.mp4</td>\n",
       "      <td>Hi and welcome back in this section. We're going to look at the types of business problems. Machine learning can help you solve. Machine learning is used all across your digital lives. Your email ...</td>\n",
       "      <td>hi and welcome back in this section. we're going to look at the types of business problems. machine learning can help you solve. machine learning is used all across your digital lives. your email ...</td>\n",
       "      <td>[{'BeginOffset': 24, 'EndOffset': 36, 'Score': 0.9996331597282907, 'Text': 'this section'}, {'BeginOffset': 61, 'EndOffset': 70, 'Score': 0.999921626630556, 'Text': 'the types'}, {'BeginOffset': 7...</td>\n",
       "      <td>[{'BeginOffset': 763, 'EndOffset': 767, 'Score': 0.5536403180912558, 'Text': 'more', 'Type': 'QUANTITY'}, {'BeginOffset': 935, 'EndOffset': 951, 'Score': 0.9698888667844483, 'Text': 'three main ty...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transcribed-input/Mod02_Sect03.txt</td>\n",
       "      <td>input/Mod02_Sect03.mp4</td>\n",
       "      <td>Hi and welcome back. This is section three and we're going to give you a quick high level overview of machine learning terminology and a typical workflow. We will cover these topics in more detail...</td>\n",
       "      <td>hi and welcome back. this is section three and we're going to give you a quick high level overview of machine learning terminology and a typical workflow. we will cover these topics in more detail...</td>\n",
       "      <td>[{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.9576599228626844, 'Text': 'section three'}, {'BeginOffset': 72, 'EndOffset': 99, 'Score': 0.9987502879769787, 'Text': 'a quick high level overview'...</td>\n",
       "      <td>[{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.6284302444453367, 'Text': 'section three', 'Type': 'OTHER'}, {'BeginOffset': 460, 'EndOffset': 468, 'Score': 0.9770625204054991, 'Text': 'one task'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    OutputFile  \\\n",
       "0  transcribed-input/Mod01_Course_Overview.txt   \n",
       "1            transcribed-input/Mod02_Intro.txt   \n",
       "2           transcribed-input/Mod02_Sect01.txt   \n",
       "3           transcribed-input/Mod02_Sect02.txt   \n",
       "4           transcribed-input/Mod02_Sect03.txt   \n",
       "\n",
       "                             Video  \\\n",
       "0  input/Mod01_Course Overview.mp4   \n",
       "1            input/Mod02_Intro.mp4   \n",
       "2           input/Mod02_Sect01.mp4   \n",
       "3           input/Mod02_Sect02.mp4   \n",
       "4           input/Mod02_Sect03.mp4   \n",
       "\n",
       "                                                                                                                                                                                             Transcription  \\\n",
       "0  Hi and welcome to Amazon Academy and Machine Learning Foundations in this module, you'll learn about the course objectives, various job roles in the machine learning domain and where you can go to...   \n",
       "1  Hi and welcome to module two of Aws Academy machine learning in this module, we're going to introduce machine learning. We'll first look at the business problems that can be solved by machine lear...   \n",
       "2  Hi and welcome to section one in this section. We're going to talk about what machine learning is. This course is an introduction to machine learning, which is also known as ML. But first we'll di...   \n",
       "3  Hi and welcome back in this section. We're going to look at the types of business problems. Machine learning can help you solve. Machine learning is used all across your digital lives. Your email ...   \n",
       "4  Hi and welcome back. This is section three and we're going to give you a quick high level overview of machine learning terminology and a typical workflow. We will cover these topics in more detail...   \n",
       "\n",
       "                                                                                                                                                                                  Transcription_normalized  \\\n",
       "0  hi and welcome to amazon academy and machine learning foundations in this module, you'll learn about the course objectives, various job roles in the machine learning domain and where you can go to...   \n",
       "1  hi and welcome to module two of aws academy machine learning in this module, we're going to introduce machine learning. we'll first look at the business problems that can be solved by machine lear...   \n",
       "2  hi and welcome to section one in this section. we're going to talk about what machine learning is. this course is an introduction to machine learning, which is also known as ml. but first we'll di...   \n",
       "3  hi and welcome back in this section. we're going to look at the types of business problems. machine learning can help you solve. machine learning is used all across your digital lives. your email ...   \n",
       "4  hi and welcome back. this is section three and we're going to give you a quick high level overview of machine learning terminology and a typical workflow. we will cover these topics in more detail...   \n",
       "\n",
       "                                                                                                                                                                                                KeyPhrases  \\\n",
       "0  [{'BeginOffset': 26, 'EndOffset': 54, 'Score': 0.832979081503515, 'Text': 'academy and machine learning'}, {'BeginOffset': 55, 'EndOffset': 66, 'Score': 0.6903008712888459, 'Text': 'foundations'},...   \n",
       "1  [{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.6214939014751475, 'Text': 'module two'}, {'BeginOffset': 33, 'EndOffset': 52, 'Score': 0.9941312955832783, 'Text': 'aws academy machine'}, {'BeginO...   \n",
       "2  [{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.9879459585199664, 'Text': 'section one'}, {'BeginOffset': 34, 'EndOffset': 46, 'Score': 0.9997940602015712, 'Text': 'this section'}, {'BeginOffset'...   \n",
       "3  [{'BeginOffset': 24, 'EndOffset': 36, 'Score': 0.9996331597282907, 'Text': 'this section'}, {'BeginOffset': 61, 'EndOffset': 70, 'Score': 0.999921626630556, 'Text': 'the types'}, {'BeginOffset': 7...   \n",
       "4  [{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.9576599228626844, 'Text': 'section three'}, {'BeginOffset': 72, 'EndOffset': 99, 'Score': 0.9987502879769787, 'Text': 'a quick high level overview'...   \n",
       "\n",
       "                                                                                                                                                                                                  Entities  \\\n",
       "0  [{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 547, 'EndOffset': 552, 'Score': 0.9491953642972518, 'Text': 'f...   \n",
       "1  [{'BeginOffset': 19, 'EndOffset': 29, 'Score': 0.8810036668646961, 'Text': 'module two', 'Type': 'OTHER'}, {'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': ...   \n",
       "2  [{'BeginOffset': 19, 'EndOffset': 30, 'Score': 0.8874841148902434, 'Text': 'section one', 'Type': 'OTHER'}, {'BeginOffset': 183, 'EndOffset': 188, 'Score': 0.9281728391309813, 'Text': 'first', 'Ty...   \n",
       "3  [{'BeginOffset': 763, 'EndOffset': 767, 'Score': 0.5536403180912558, 'Text': 'more', 'Type': 'QUANTITY'}, {'BeginOffset': 935, 'EndOffset': 951, 'Score': 0.9698888667844483, 'Text': 'three main ty...   \n",
       "4  [{'BeginOffset': 30, 'EndOffset': 43, 'Score': 0.6284302444453367, 'Text': 'section three', 'Type': 'OTHER'}, {'BeginOffset': 460, 'EndOffset': 468, 'Score': 0.9770625204054991, 'Text': 'one task'...   \n",
       "\n",
       "  location  \\\n",
       "0       []   \n",
       "1       []   \n",
       "2       []   \n",
       "3       []   \n",
       "4       []   \n",
       "\n",
       "                                                                                                                                                                                              organization  \n",
       "0  [{'BeginOffset': 19, 'EndOffset': 33, 'Score': 0.9329590519475547, 'Text': 'amazon academy', 'Type': 'ORGANIZATION'}, {'BeginOffset': 562, 'EndOffset': 573, 'Score': 0.9465551617295611, 'Text': 'a...  \n",
       "1                                                                                               [{'BeginOffset': 33, 'EndOffset': 36, 'Score': 0.9779731824490148, 'Text': 'aws', 'Type': 'ORGANIZATION'}]  \n",
       "2                                                                                                                                                                                                       []  \n",
       "3                                                                                                                                                                                                       []  \n",
       "4                                                                                                                                                                                                       []  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OutputFile</th>\n",
       "      <th>Video</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>Transcription_normalized</th>\n",
       "      <th>KeyPhrases</th>\n",
       "      <th>Entities</th>\n",
       "      <th>location</th>\n",
       "      <th>organization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcribed-input/Mod01_Course_Overview.txt</td>\n",
       "      <td>input/Mod01_Course Overview.mp4</td>\n",
       "      <td>Hi and welcome to Amazon Academy and Machine L...</td>\n",
       "      <td>hi and welcome to amazon academy and machine l...</td>\n",
       "      <td>[{'BeginOffset': 26, 'EndOffset': 54, 'Score':...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score':...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 33, 'Score':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transcribed-input/Mod02_Intro.txt</td>\n",
       "      <td>input/Mod02_Intro.mp4</td>\n",
       "      <td>Hi and welcome to module two of Aws Academy ma...</td>\n",
       "      <td>hi and welcome to module two of aws academy ma...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 29, 'Score':...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 29, 'Score':...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'BeginOffset': 33, 'EndOffset': 36, 'Score':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transcribed-input/Mod02_Sect01.txt</td>\n",
       "      <td>input/Mod02_Sect01.mp4</td>\n",
       "      <td>Hi and welcome to section one in this section....</td>\n",
       "      <td>hi and welcome to section one in this section....</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 30, 'Score':...</td>\n",
       "      <td>[{'BeginOffset': 19, 'EndOffset': 30, 'Score':...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transcribed-input/Mod02_Sect02.txt</td>\n",
       "      <td>input/Mod02_Sect02.mp4</td>\n",
       "      <td>Hi and welcome back in this section. We're goi...</td>\n",
       "      <td>hi and welcome back in this section. we're goi...</td>\n",
       "      <td>[{'BeginOffset': 24, 'EndOffset': 36, 'Score':...</td>\n",
       "      <td>[{'BeginOffset': 763, 'EndOffset': 767, 'Score...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transcribed-input/Mod02_Sect03.txt</td>\n",
       "      <td>input/Mod02_Sect03.mp4</td>\n",
       "      <td>Hi and welcome back. This is section three and...</td>\n",
       "      <td>hi and welcome back. this is section three and...</td>\n",
       "      <td>[{'BeginOffset': 30, 'EndOffset': 43, 'Score':...</td>\n",
       "      <td>[{'BeginOffset': 30, 'EndOffset': 43, 'Score':...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    OutputFile  \\\n",
       "0  transcribed-input/Mod01_Course_Overview.txt   \n",
       "1            transcribed-input/Mod02_Intro.txt   \n",
       "2           transcribed-input/Mod02_Sect01.txt   \n",
       "3           transcribed-input/Mod02_Sect02.txt   \n",
       "4           transcribed-input/Mod02_Sect03.txt   \n",
       "\n",
       "                             Video  \\\n",
       "0  input/Mod01_Course Overview.mp4   \n",
       "1            input/Mod02_Intro.mp4   \n",
       "2           input/Mod02_Sect01.mp4   \n",
       "3           input/Mod02_Sect02.mp4   \n",
       "4           input/Mod02_Sect03.mp4   \n",
       "\n",
       "                                       Transcription  \\\n",
       "0  Hi and welcome to Amazon Academy and Machine L...   \n",
       "1  Hi and welcome to module two of Aws Academy ma...   \n",
       "2  Hi and welcome to section one in this section....   \n",
       "3  Hi and welcome back in this section. We're goi...   \n",
       "4  Hi and welcome back. This is section three and...   \n",
       "\n",
       "                            Transcription_normalized  \\\n",
       "0  hi and welcome to amazon academy and machine l...   \n",
       "1  hi and welcome to module two of aws academy ma...   \n",
       "2  hi and welcome to section one in this section....   \n",
       "3  hi and welcome back in this section. we're goi...   \n",
       "4  hi and welcome back. this is section three and...   \n",
       "\n",
       "                                          KeyPhrases  \\\n",
       "0  [{'BeginOffset': 26, 'EndOffset': 54, 'Score':...   \n",
       "1  [{'BeginOffset': 19, 'EndOffset': 29, 'Score':...   \n",
       "2  [{'BeginOffset': 19, 'EndOffset': 30, 'Score':...   \n",
       "3  [{'BeginOffset': 24, 'EndOffset': 36, 'Score':...   \n",
       "4  [{'BeginOffset': 30, 'EndOffset': 43, 'Score':...   \n",
       "\n",
       "                                            Entities location  \\\n",
       "0  [{'BeginOffset': 19, 'EndOffset': 33, 'Score':...       []   \n",
       "1  [{'BeginOffset': 19, 'EndOffset': 29, 'Score':...       []   \n",
       "2  [{'BeginOffset': 19, 'EndOffset': 30, 'Score':...       []   \n",
       "3  [{'BeginOffset': 763, 'EndOffset': 767, 'Score...       []   \n",
       "4  [{'BeginOffset': 30, 'EndOffset': 43, 'Score':...       []   \n",
       "\n",
       "                                        organization  \n",
       "0  [{'BeginOffset': 19, 'EndOffset': 33, 'Score':...  \n",
       "1  [{'BeginOffset': 33, 'EndOffset': 36, 'Score':...  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 50)\n",
    "mergedDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................ready!\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "alive = es_client.describe_elasticsearch_domain(DomainName='nlp-lab')\n",
    "while alive['DomainStatus']['Processing']:\n",
    "    print('.', end='')\n",
    "    sleep(10)\n",
    "    alive = es_client.describe_elasticsearch_domain(DomainName='nlp-lab')\n",
    "    \n",
    "print('ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(60)\n",
    "es_domain = es_client.describe_elasticsearch_domain(DomainName='nlp-lab')\n",
    "es_endpoint = es_domain['DomainStatus']['Endpoint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenSearch client using the following:\n",
    "region= 'us-east-1' # us-east-1\n",
    "service = 'es' # IMPORTANT: this is key difference while signing the request for proxy endpoint.\n",
    "credentials = boto3.Session().get_credentials()\n",
    "\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "es = OpenSearch(\n",
    "    hosts = [{'host': es_endpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'input/Mod02_Sect02.mp4', 'transcription': \"Hi and welcome back in this section. We're going to look at the types of business problems. Machine learning can help you solve. Machine learning is used all across your digital lives. Your email spam filter is the result of a machine learning program that was trained with examples of spam and regular email messages based on books. You're reading or products you bought machine learning programs can predict other books or products you're likely to be interested in. Again, the machine learning program was trained with data from other readers habits and purchases. When detecting credit card fraud, the machine learning program was trained on examples of transactions that turned out to be fraud along with normal transactions. You can probably think of many more examples from social media applications using facial detection to group your photos to detecting brain tumors in brain scans or finding anomalies in x rays. There are three main types of machine learning. There's supervised learning where a model uses known inputs and outputs to generalize future outputs. There's unsupervised learning where the model doesn't know inputs or outputs. So it finds patterns in the data without help. And there's reinforcement learning where the model interacts with its environment and learns to take actions that will maximize rewards. It's important to know the different types of ML because the type will guide you towards selecting algorithms that make sense for solving your business problem. Let's look more into each of these types. Supervised learning is a popular type of ML because it's widely applicable, it's called supervised learning because there needs to be a supervisor, a teacher who can show the right answers, so to speak. Like any student, a supervised algorithm needs to learn. By example, essentially it needs a teacher who uses training data to help it determine the patterns and relationships between the inputs and outputs. If you want to build an application to detect credit card fraud, you'd need training data that includes examples of fraud and examples of normal transactions within supervised learning. There are different types of problems, classification and regression. There are two subtypes of classification problems. The first is binary classification. Think back to the example with identifying fraudulent transactions. The target variable in this example, is limited to two options, fraudulent or not fraudulent. This is a binary classification problem. There are also multi class classification problems. These ML problems classify an observation into one of three or more categories, say that you have an ML model that predicts why a customer is calling your store. So you can reduce the number of transfers needed before the customer gets to the correct customer support department. In this case, the different customer support departments represent the variety of potential target variables which could be many different departments much more than just two. There are also regression problems in a regression problem. You're no longer mapping an input to a defined number of categories. Instead you're mapping an input to a continuous value like an integer. One example of an ML regression problem is predicting the price of a company's stock. Computer vision is a good example of supervised learning. Is this a cat or a dog? Is there a tumor in this X ray? Computer vision is often built with deep learning models. It automates the extraction analysis, classification and understanding of useful information from a single image or a sequence of images. Computer vision enables machines to identify people, places and things and images with accuracy at or above human levels and with greater speed and efficiency. The image data can take many forms such as single images, video sequences, views from multiple cameras or three dimensional data. You'll learn more about computer vision. Later. In this course, we now discuss unsupervised machine learning. Sometimes all you have is the data, there's no supervisor in the room in unsupervised learning labels aren't provided like they are with supervised learning. You don't know all the variables and patterns in these instances. The machine has to uncover and create the labels itself. These models use the data they're presented with to detect emerging properties of the entire data set, then they construct patterns from those properties. Clustering is a common subcategory of unsupervised learning. This kind of algorithm groups data into different clusters based on similar features. It does this to better understand the attributes of a specific cluster. For example, by analyzing customer purchasing habits, unsupervised algorithms can identify groups of customers that are associated with the size tier of a company. The advantage of unsupervised algorithms is that they enable you to see patterns in the data that you weren't aware of before natural language processing is also known as N LP. This is another area of machine learning that's experiencing growth. If you've ever used Alexa or any other voice assistant, they'll use N LP to try and answer your question. N LP isn't just about speech, it's also about written text. N LP shows up in many applications. For example, N LP is used with chat or call center bots which are automated systems that help you get your bank balance or order food from a restaurant. You can use N LP and translation tools which convert text between languages. For example, you might use applications that translate menus in real time. NLP is also used in voice to text translations which converts spoken words into text. And finally NP can be used in sentiment analysis which you can use to analyze the sentiment of comments and reviews of products, music and movies. These sentiments could be used to give the movie an audience rating, you'll learn more about N LP later in this course. Another kind of machine learning that's been gaining popularity recently is reinforcement learning. Unlike other machine learning reinforcement learning continuously improves its model by mining feedback from previous iterations. In reinforcement learning, an agent continuously learns through trial and error as it interacts in an environment. Reinforcement learning is broadly useful when the reward of a desired outcome is known. But the path to achieving it isn't and that path requires a lot of trial and error to discover. Take the example of Aws Deep Racer in the Aws Deep Racer simulator. The agent is the virtual car. The environment is a virtual racetrack. The actions are throttle and steering inputs to the car. And the goal is completing the racetrack as quickly as possible. Without deviating from the track. The car needs to learn the desired driving behavior to reach the goal of completing the track for the car to learn this Aws Deep racer teams use rewards to incentivize their model to learn the desired driving behavior. In reinforcement learning the thing driving, the learning is called the agent. In this case, it's the Aws deep racer car. The environment is the place where the agent learns, which in this example would be the marked racetrack. When the agent does something in the environment that provokes a response such as crossing a boundary, it shouldn't cross, that's called an action that response is called a reward or penalty. Depending on whether the agent did something to be reinforced or discouraged in the model. As the agent moves within the environment, its actions should start receiving more rewards and fewer penalties until it meets the desired business outcome. Self driving vehicles, bring together many machine and deep learning algorithms and models to solve the problem of driving from point A to point B. Two of its main tasks are the continuous detection of the environment and forecasting changes. These involve detecting objects and localizing and predicting the movement of the detected objects. The outputs of these findings act as inputs to other systems that make decisions on what they should do with the vehicle's various controls. There are use cases in self driving vehicles that require real time responses to the environment. For example, if a previously hidden pedestrian walks out from behind an obstacle, the vehicle brakes need to be applied immediately, there can be no latency or room for error with these actions. Not every problem should be solved with machine learning. Sometimes regular programming will work well for your needs. If you're interested in exploring a potential machine learning solution, look for the existence of large data sets and a large number of variables. Machine learning is often the best choice if you're uncertain of the business logic or procedures needed to obtain an answer or accomplish a task. Machine learning systems can be complex. The supporting infrastructure management support and technical expertise need to be in place to help ensure the project's success. Here are the key takeaways for this section where we explored some machine learning applications that are already part of everyday life. First, machine learning problems can be grouped into three categories. Supervised learning is where you have training data where you already know the answer, unsupervised learning is where you have data but are looking for insights within the data reinforcement learning is where the model learns based on experience and feedback. Most business problems are supervised learning problems. That's it. For this section, we'll see you in the next video.\", 'keyphrases': [{'BeginOffset': 24, 'EndOffset': 36, 'Score': 0.9996331597282907, 'Text': 'this section'}, {'BeginOffset': 61, 'EndOffset': 70, 'Score': 0.999921626630556, 'Text': 'the types'}, {'BeginOffset': 74, 'EndOffset': 91, 'Score': 0.9967263828498905, 'Text': 'business problems'}, {'BeginOffset': 93, 'EndOffset': 109, 'Score': 0.9957156408842702, 'Text': 'machine learning'}, {'BeginOffset': 130, 'EndOffset': 146, 'Score': 0.9956345590473414, 'Text': 'machine learning'}, {'BeginOffset': 166, 'EndOffset': 184, 'Score': 0.9999054792172132, 'Text': 'your digital lives'}, {'BeginOffset': 186, 'EndOffset': 208, 'Score': 0.9984253858124604, 'Text': 'your email spam filter'}, {'BeginOffset': 212, 'EndOffset': 222, 'Score': 0.9999101848107042, 'Text': 'the result'}, {'BeginOffset': 226, 'EndOffset': 244, 'Score': 0.8110439557949295, 'Text': 'a machine learning'}, {'BeginOffset': 245, 'EndOffset': 252, 'Score': 0.7080841031374195, 'Text': 'program'}, {'BeginOffset': 275, 'EndOffset': 283, 'Score': 0.999898086446007, 'Text': 'examples'}, {'BeginOffset': 287, 'EndOffset': 291, 'Score': 0.9999408756865238, 'Text': 'spam'}, {'BeginOffset': 296, 'EndOffset': 318, 'Score': 0.8328285917566294, 'Text': 'regular email messages'}, {'BeginOffset': 328, 'EndOffset': 333, 'Score': 0.9999187058723578, 'Text': 'books'}, {'BeginOffset': 342, 'EndOffset': 349, 'Score': 0.5047988228251223, 'Text': 'reading'}, {'BeginOffset': 353, 'EndOffset': 361, 'Score': 0.6006750740812093, 'Text': 'products'}, {'BeginOffset': 373, 'EndOffset': 389, 'Score': 0.8174317166856087, 'Text': 'machine learning'}, {'BeginOffset': 390, 'EndOffset': 398, 'Score': 0.6880348272591014, 'Text': 'programs'}, {'BeginOffset': 411, 'EndOffset': 434, 'Score': 0.9975682355444748, 'Text': 'other books or products'}, {'BeginOffset': 477, 'EndOffset': 505, 'Score': 0.974313316551069, 'Text': 'the machine learning program'}, {'BeginOffset': 523, 'EndOffset': 527, 'Score': 0.999867099307243, 'Text': 'data'}, {'BeginOffset': 533, 'EndOffset': 567, 'Score': 0.9609193018566236, 'Text': 'other readers habits and purchases'}, {'BeginOffset': 584, 'EndOffset': 601, 'Score': 0.9977656385745808, 'Text': 'credit card fraud'}, {'BeginOffset': 603, 'EndOffset': 631, 'Score': 0.9281574304155338, 'Text': 'the machine learning program'}, {'BeginOffset': 647, 'EndOffset': 655, 'Score': 0.999911912093075, 'Text': 'examples'}, {'BeginOffset': 659, 'EndOffset': 671, 'Score': 0.9999800924450729, 'Text': 'transactions'}, {'BeginOffset': 694, 'EndOffset': 699, 'Score': 0.9671567565634945, 'Text': 'fraud'}, {'BeginOffset': 711, 'EndOffset': 730, 'Score': 0.9999451667363283, 'Text': 'normal transactions'}, {'BeginOffset': 758, 'EndOffset': 776, 'Score': 0.9983656342207914, 'Text': 'many more examples'}, {'BeginOffset': 782, 'EndOffset': 807, 'Score': 0.993515162786431, 'Text': 'social media applications'}, {'BeginOffset': 814, 'EndOffset': 830, 'Score': 0.9976414268565674, 'Text': 'facial detection'}, {'BeginOffset': 834, 'EndOffset': 839, 'Score': 0.9322801131601468, 'Text': 'group'}, {'BeginOffset': 840, 'EndOffset': 851, 'Score': 0.935803540582291, 'Text': 'your photos'}, {'BeginOffset': 865, 'EndOffset': 877, 'Score': 0.9990651227415344, 'Text': 'brain tumors'}, {'BeginOffset': 881, 'EndOffset': 892, 'Score': 0.9976420734740197, 'Text': 'brain scans'}, {'BeginOffset': 904, 'EndOffset': 913, 'Score': 0.7024100512060648, 'Text': 'anomalies'}, {'BeginOffset': 917, 'EndOffset': 923, 'Score': 0.9973434031478041, 'Text': 'x rays'}, {'BeginOffset': 935, 'EndOffset': 951, 'Score': 0.999867503973829, 'Text': 'three main types'}, {'BeginOffset': 955, 'EndOffset': 971, 'Score': 0.8705468243271747, 'Text': 'machine learning'}, {'BeginOffset': 1007, 'EndOffset': 1014, 'Score': 0.9999646557123755, 'Text': 'a model'}, {'BeginOffset': 1020, 'EndOffset': 1044, 'Score': 0.9985522942370656, 'Text': 'known inputs and outputs'}, {'BeginOffset': 1059, 'EndOffset': 1073, 'Score': 0.9966712105564268, 'Text': 'future outputs'}, {'BeginOffset': 1111, 'EndOffset': 1120, 'Score': 0.9999790196206895, 'Text': 'the model'}, {'BeginOffset': 1134, 'EndOffset': 1151, 'Score': 0.9996532012647295, 'Text': 'inputs or outputs'}, {'BeginOffset': 1165, 'EndOffset': 1173, 'Score': 0.9994782548833284, 'Text': 'patterns'}, {'BeginOffset': 1177, 'EndOffset': 1185, 'Score': 0.9999626890297247, 'Text': 'the data'}, {'BeginOffset': 1212, 'EndOffset': 1225, 'Score': 0.9361271261847776, 'Text': 'reinforcement'}, {'BeginOffset': 1241, 'EndOffset': 1250, 'Score': 0.9999638808887751, 'Text': 'the model'}, {'BeginOffset': 1266, 'EndOffset': 1281, 'Score': 0.9999805692692142, 'Text': 'its environment'}, {'BeginOffset': 1301, 'EndOffset': 1308, 'Score': 0.9999605432821229, 'Text': 'actions'}, {'BeginOffset': 1328, 'EndOffset': 1335, 'Score': 0.9971982282909261, 'Text': 'rewards'}, {'BeginOffset': 1360, 'EndOffset': 1379, 'Score': 0.9999109216222508, 'Text': 'the different types'}, {'BeginOffset': 1383, 'EndOffset': 1385, 'Score': 0.9965926084548541, 'Text': 'ml'}, {'BeginOffset': 1394, 'EndOffset': 1402, 'Score': 0.9999799136538133, 'Text': 'the type'}, {'BeginOffset': 1436, 'EndOffset': 1446, 'Score': 0.9937145124639346, 'Text': 'algorithms'}, {'BeginOffset': 1457, 'EndOffset': 1462, 'Score': 0.9986644961410622, 'Text': 'sense'}, {'BeginOffset': 1475, 'EndOffset': 1496, 'Score': 0.9996086511487797, 'Text': 'your business problem'}, {'BeginOffset': 1527, 'EndOffset': 1538, 'Score': 0.999905538324842, 'Text': 'these types'}, {'BeginOffset': 1540, 'EndOffset': 1559, 'Score': 0.8892718745417849, 'Text': 'supervised learning'}, {'BeginOffset': 1563, 'EndOffset': 1577, 'Score': 0.9999584374482156, 'Text': 'a popular type'}, {'BeginOffset': 1581, 'EndOffset': 1583, 'Score': 0.9982125579351282, 'Text': 'ml'}, {'BeginOffset': 1674, 'EndOffset': 1686, 'Score': 0.9999174546083852, 'Text': 'a supervisor'}, {'BeginOffset': 1688, 'EndOffset': 1697, 'Score': 0.9998868243439616, 'Text': 'a teacher'}, {'BeginOffset': 1711, 'EndOffset': 1728, 'Score': 0.9998222930861388, 'Text': 'the right answers'}, {'BeginOffset': 1748, 'EndOffset': 1759, 'Score': 0.9999289580834771, 'Text': 'any student'}, {'BeginOffset': 1761, 'EndOffset': 1783, 'Score': 0.9999403199206576, 'Text': 'a supervised algorithm'}, {'BeginOffset': 1833, 'EndOffset': 1842, 'Score': 0.9999489221189235, 'Text': 'a teacher'}, {'BeginOffset': 1852, 'EndOffset': 1865, 'Score': 0.9505085701964407, 'Text': 'training data'}, {'BeginOffset': 1887, 'EndOffset': 1917, 'Score': 0.9998717222284615, 'Text': 'the patterns and relationships'}, {'BeginOffset': 1926, 'EndOffset': 1948, 'Score': 0.9992212808197605, 'Text': 'the inputs and outputs'}, {'BeginOffset': 1971, 'EndOffset': 1985, 'Score': 0.9998343931557089, 'Text': 'an application'}, {'BeginOffset': 1996, 'EndOffset': 2013, 'Score': 0.9980161073745762, 'Text': 'credit card fraud'}, {'BeginOffset': 2026, 'EndOffset': 2039, 'Score': 0.9966393173813919, 'Text': 'training data'}, {'BeginOffset': 2054, 'EndOffset': 2062, 'Score': 0.9999070253984754, 'Text': 'examples'}, {'BeginOffset': 2066, 'EndOffset': 2084, 'Score': 0.9890009317361409, 'Text': 'fraud and examples'}, {'BeginOffset': 2088, 'EndOffset': 2107, 'Score': 0.9999290162997366, 'Text': 'normal transactions'}, {'BeginOffset': 2115, 'EndOffset': 2134, 'Score': 0.9081856610119673, 'Text': 'supervised learning'}, {'BeginOffset': 2146, 'EndOffset': 2161, 'Score': 0.9994867726059444, 'Text': 'different types'}, {'BeginOffset': 2165, 'EndOffset': 2204, 'Score': 0.9971876891435562, 'Text': 'problems, classification and regression'}, {'BeginOffset': 2216, 'EndOffset': 2228, 'Score': 0.9996271222842917, 'Text': 'two subtypes'}, {'BeginOffset': 2232, 'EndOffset': 2255, 'Score': 0.9995374234835324, 'Text': 'classification problems'}, {'BeginOffset': 2257, 'EndOffset': 2266, 'Score': 0.9988243027980943, 'Text': 'the first'}, {'BeginOffset': 2270, 'EndOffset': 2291, 'Score': 0.9782232008238724, 'Text': 'binary classification'}, {'BeginOffset': 2307, 'EndOffset': 2318, 'Score': 0.9999249635525644, 'Text': 'the example'}, {'BeginOffset': 2336, 'EndOffset': 2359, 'Score': 0.9995350890369761, 'Text': 'fraudulent transactions'}, {'BeginOffset': 2361, 'EndOffset': 2380, 'Score': 0.9996654942407577, 'Text': 'the target variable'}, {'BeginOffset': 2384, 'EndOffset': 2396, 'Score': 0.9999231176131789, 'Text': 'this example'}, {'BeginOffset': 2412, 'EndOffset': 2423, 'Score': 0.9866421039174439, 'Text': 'two options'}, {'BeginOffset': 2463, 'EndOffset': 2494, 'Score': 0.999422530816483, 'Text': 'a binary classification problem'}, {'BeginOffset': 2511, 'EndOffset': 2546, 'Score': 0.9908997442497837, 'Text': 'multi class classification problems'}, {'BeginOffset': 2548, 'EndOffset': 2565, 'Score': 0.9920733065439797, 'Text': 'these ml problems'}, {'BeginOffset': 2575, 'EndOffset': 2589, 'Score': 0.9998156802681201, 'Text': 'an observation'}, {'BeginOffset': 2602, 'EndOffset': 2626, 'Score': 0.9906569326416332, 'Text': 'three or more categories'}, {'BeginOffset': 2646, 'EndOffset': 2657, 'Score': 0.9997864416985659, 'Text': 'an ml model'}, {'BeginOffset': 2676, 'EndOffset': 2686, 'Score': 0.999966384240223, 'Text': 'a customer'}, {'BeginOffset': 2698, 'EndOffset': 2708, 'Score': 0.9999489219347603, 'Text': 'your store'}, {'BeginOffset': 2728, 'EndOffset': 2738, 'Score': 0.9999448689841348, 'Text': 'the number'}, {'BeginOffset': 2742, 'EndOffset': 2751, 'Score': 0.9994998097440204, 'Text': 'transfers'}, {'BeginOffset': 2766, 'EndOffset': 2778, 'Score': 0.9998640626907509, 'Text': 'the customer'}, {'BeginOffset': 2787, 'EndOffset': 2826, 'Score': 0.9934760328480164, 'Text': 'the correct customer support department'}, {'BeginOffset': 2831, 'EndOffset': 2840, 'Score': 0.9994575744082865, 'Text': 'this case'}, {'BeginOffset': 2842, 'EndOffset': 2884, 'Score': 0.9742910562221989, 'Text': 'the different customer support departments'}, {'BeginOffset': 2895, 'EndOffset': 2906, 'Score': 0.9998661467895354, 'Text': 'the variety'}, {'BeginOffset': 2910, 'EndOffset': 2936, 'Score': 0.999489738460604, 'Text': 'potential target variables'}, {'BeginOffset': 2952, 'EndOffset': 2978, 'Score': 0.9954415991295946, 'Text': 'many different departments'}, {'BeginOffset': 2994, 'EndOffset': 3002, 'Score': 0.8167586569762685, 'Text': 'just two'}, {'BeginOffset': 3019, 'EndOffset': 3038, 'Score': 0.9978297037784264, 'Text': 'regression problems'}, {'BeginOffset': 3042, 'EndOffset': 3062, 'Score': 0.9998998014500824, 'Text': 'a regression problem'}, {'BeginOffset': 3089, 'EndOffset': 3097, 'Score': 0.9997241868524783, 'Text': 'an input'}, {'BeginOffset': 3101, 'EndOffset': 3117, 'Score': 0.9999022990402321, 'Text': 'a defined number'}, {'BeginOffset': 3121, 'EndOffset': 3131, 'Score': 0.9999243078326592, 'Text': 'categories'}, {'BeginOffset': 3156, 'EndOffset': 3164, 'Score': 0.9997474197176126, 'Text': 'an input'}, {'BeginOffset': 3168, 'EndOffset': 3186, 'Score': 0.9999391680689469, 'Text': 'a continuous value'}, {'BeginOffset': 3192, 'EndOffset': 3202, 'Score': 0.9999565508175022, 'Text': 'an integer'}, {'BeginOffset': 3204, 'EndOffset': 3215, 'Score': 0.999608188713329, 'Text': 'one example'}, {'BeginOffset': 3219, 'EndOffset': 3243, 'Score': 0.9977435501058888, 'Text': 'an ml regression problem'}, {'BeginOffset': 3258, 'EndOffset': 3267, 'Score': 0.9999728211379141, 'Text': 'the price'}, {'BeginOffset': 3271, 'EndOffset': 3280, 'Score': 0.9999663844669069, 'Text': 'a company'}, {'BeginOffset': 3283, 'EndOffset': 3288, 'Score': 0.9999291948427158, 'Text': 'stock'}, {'BeginOffset': 3290, 'EndOffset': 3305, 'Score': 0.9994876571559561, 'Text': 'computer vision'}, {'BeginOffset': 3309, 'EndOffset': 3323, 'Score': 0.9999436571148272, 'Text': 'a good example'}, {'BeginOffset': 3327, 'EndOffset': 3346, 'Score': 0.9784871792356301, 'Text': 'supervised learning'}, {'BeginOffset': 3356, 'EndOffset': 3361, 'Score': 0.9914862929948279, 'Text': 'a cat'}, {'BeginOffset': 3365, 'EndOffset': 3370, 'Score': 0.9996010470388863, 'Text': 'a dog'}, {'BeginOffset': 3381, 'EndOffset': 3388, 'Score': 0.9868085792469057, 'Text': 'a tumor'}, {'BeginOffset': 3392, 'EndOffset': 3402, 'Score': 0.9972747793328121, 'Text': 'this x ray'}, {'BeginOffset': 3404, 'EndOffset': 3419, 'Score': 0.9991038135853233, 'Text': 'computer vision'}, {'BeginOffset': 3440, 'EndOffset': 3460, 'Score': 0.9409664113342294, 'Text': 'deep learning models'}, {'BeginOffset': 3475, 'EndOffset': 3532, 'Score': 0.994238142839322, 'Text': 'the extraction analysis, classification and understanding'}, {'BeginOffset': 3536, 'EndOffset': 3554, 'Score': 0.9997850526594048, 'Text': 'useful information'}, {'BeginOffset': 3560, 'EndOffset': 3574, 'Score': 0.9999633644285627, 'Text': 'a single image'}, {'BeginOffset': 3578, 'EndOffset': 3588, 'Score': 0.9999302079649585, 'Text': 'a sequence'}, {'BeginOffset': 3592, 'EndOffset': 3598, 'Score': 0.9999754434898208, 'Text': 'images'}, {'BeginOffset': 3600, 'EndOffset': 3615, 'Score': 0.9998003767853976, 'Text': 'computer vision'}, {'BeginOffset': 3624, 'EndOffset': 3632, 'Score': 0.9997512720879966, 'Text': 'machines'}, {'BeginOffset': 3645, 'EndOffset': 3681, 'Score': 0.9784874525567361, 'Text': 'people, places and things and images'}, {'BeginOffset': 3687, 'EndOffset': 3695, 'Score': 0.9997399529669438, 'Text': 'accuracy'}, {'BeginOffset': 3708, 'EndOffset': 3720, 'Score': 0.9995431304717975, 'Text': 'human levels'}, {'BeginOffset': 3730, 'EndOffset': 3758, 'Score': 0.9959874106929592, 'Text': 'greater speed and efficiency'}, {'BeginOffset': 3760, 'EndOffset': 3774, 'Score': 0.9994341963330584, 'Text': 'the image data'}, {'BeginOffset': 3784, 'EndOffset': 3794, 'Score': 0.9997806488130104, 'Text': 'many forms'}, {'BeginOffset': 3803, 'EndOffset': 3816, 'Score': 0.9994158028432526, 'Text': 'single images'}, {'BeginOffset': 3818, 'EndOffset': 3833, 'Score': 0.9996918197865057, 'Text': 'video sequences'}, {'BeginOffset': 3835, 'EndOffset': 3840, 'Score': 0.9860076518884388, 'Text': 'views'}, {'BeginOffset': 3846, 'EndOffset': 3862, 'Score': 0.9999655497235627, 'Text': 'multiple cameras'}, {'BeginOffset': 3866, 'EndOffset': 3888, 'Score': 0.9997332424409491, 'Text': 'three dimensional data'}, {'BeginOffset': 3914, 'EndOffset': 3929, 'Score': 0.9973515586735147, 'Text': 'computer vision'}, {'BeginOffset': 3941, 'EndOffset': 3952, 'Score': 0.9999501138010473, 'Text': 'this course'}, {'BeginOffset': 3969, 'EndOffset': 3989, 'Score': 0.9984871679217067, 'Text': 'unsupervised machine'}, {'BeginOffset': 4026, 'EndOffset': 4034, 'Score': 0.9998792584076709, 'Text': 'the data'}, {'BeginOffset': 4044, 'EndOffset': 4057, 'Score': 0.9978323456694387, 'Text': 'no supervisor'}, {'BeginOffset': 4061, 'EndOffset': 4069, 'Score': 0.9999158453298094, 'Text': 'the room'}, {'BeginOffset': 4073, 'EndOffset': 4101, 'Score': 0.8580011465274894, 'Text': 'unsupervised learning labels'}, {'BeginOffset': 4137, 'EndOffset': 4156, 'Score': 0.8670510025955794, 'Text': 'supervised learning'}, {'BeginOffset': 4173, 'EndOffset': 4203, 'Score': 0.9896375142660507, 'Text': 'all the variables and patterns'}, {'BeginOffset': 4207, 'EndOffset': 4222, 'Score': 0.9998988036225164, 'Text': 'these instances'}, {'BeginOffset': 4224, 'EndOffset': 4235, 'Score': 0.9999630465661002, 'Text': 'the machine'}, {'BeginOffset': 4262, 'EndOffset': 4272, 'Score': 0.9991547228586699, 'Text': 'the labels'}, {'BeginOffset': 4281, 'EndOffset': 4293, 'Score': 0.9998417833087169, 'Text': 'these models'}, {'BeginOffset': 4298, 'EndOffset': 4306, 'Score': 0.9999033342918591, 'Text': 'the data'}, {'BeginOffset': 4340, 'EndOffset': 4359, 'Score': 0.9980246520790329, 'Text': 'emerging properties'}, {'BeginOffset': 4363, 'EndOffset': 4382, 'Score': 0.9367493083895223, 'Text': 'the entire data set'}, {'BeginOffset': 4404, 'EndOffset': 4412, 'Score': 0.9998114464648237, 'Text': 'patterns'}, {'BeginOffset': 4418, 'EndOffset': 4434, 'Score': 0.9998248580719166, 'Text': 'those properties'}, {'BeginOffset': 4436, 'EndOffset': 4446, 'Score': 0.9429653014767069, 'Text': 'clustering'}, {'BeginOffset': 4450, 'EndOffset': 4470, 'Score': 0.9999585567798815, 'Text': 'a common subcategory'}, {'BeginOffset': 4474, 'EndOffset': 4495, 'Score': 0.9429359426666658, 'Text': 'unsupervised learning'}, {'BeginOffset': 4497, 'EndOffset': 4506, 'Score': 0.9998387987574482, 'Text': 'this kind'}, {'BeginOffset': 4510, 'EndOffset': 4531, 'Score': 0.8621443997013919, 'Text': 'algorithm groups data'}, {'BeginOffset': 4537, 'EndOffset': 4555, 'Score': 0.9998458315693707, 'Text': 'different clusters'}, {'BeginOffset': 4565, 'EndOffset': 4581, 'Score': 0.9999450475426769, 'Text': 'similar features'}, {'BeginOffset': 4617, 'EndOffset': 4631, 'Score': 0.9998569165910781, 'Text': 'the attributes'}, {'BeginOffset': 4635, 'EndOffset': 4653, 'Score': 0.9999494188546773, 'Text': 'a specific cluster'}, {'BeginOffset': 4681, 'EndOffset': 4707, 'Score': 0.6709132806857865, 'Text': 'customer purchasing habits'}, {'BeginOffset': 4709, 'EndOffset': 4732, 'Score': 0.9986161179567187, 'Text': 'unsupervised algorithms'}, {'BeginOffset': 4746, 'EndOffset': 4752, 'Score': 0.9999570864969811, 'Text': 'groups'}, {'BeginOffset': 4756, 'EndOffset': 4765, 'Score': 0.9999874831811497, 'Text': 'customers'}, {'BeginOffset': 4791, 'EndOffset': 4804, 'Score': 0.9991386804980673, 'Text': 'the size tier'}, {'BeginOffset': 4808, 'EndOffset': 4817, 'Score': 0.99998539708325, 'Text': 'a company'}, {'BeginOffset': 4819, 'EndOffset': 4832, 'Score': 0.9999801520485329, 'Text': 'the advantage'}, {'BeginOffset': 4836, 'EndOffset': 4859, 'Score': 0.9996132104389045, 'Text': 'unsupervised algorithms'}, {'BeginOffset': 4891, 'EndOffset': 4899, 'Score': 0.9995662663974494, 'Text': 'patterns'}, {'BeginOffset': 4903, 'EndOffset': 4911, 'Score': 0.9999569082059429, 'Text': 'the data'}, {'BeginOffset': 4945, 'EndOffset': 4972, 'Score': 0.9990957578076357, 'Text': 'natural language processing'}, {'BeginOffset': 4990, 'EndOffset': 4994, 'Score': 0.9311300813158246, 'Text': 'n lp'}], 'location': [], 'organization': []}\n"
     ]
    }
   ],
   "source": [
    "transcription = mergedDf.iloc[3,2]\n",
    "keyphrases = mergedDf.iloc[3,4]\n",
    "location = mergedDf.iloc[3,6]\n",
    "organization = mergedDf.iloc[3,7]\n",
    "movie_name = mergedDf.iloc[3,1]\n",
    "\n",
    "document = {\"name\": movie_name, \"transcription\": transcription, \"keyphrases\": keyphrases, \"location\":location, \"organization\": organization}\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import helpers\n",
    "\n",
    "def gendata(start, stop):    \n",
    "    if stop>mergedDf.shape[0]:\n",
    "        stop = mergedDf.shape[0]\n",
    "    for i in range(start, stop):\n",
    "        yield {\n",
    "            \"_index\":'movies',\n",
    "            \"_type\": \"_doc\", \n",
    "            \"_id\":i, \n",
    "            \"_source\": {\"name\": mergedDf.iloc[i,1], \"transcription\": mergedDf.iloc[i,2], \"keyphrases\": mergedDf.iloc[i,4], \"location\":mergedDf.iloc[i,6], \"organization\": mergedDf.iloc[i,7]}\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to get some up to date credentials for the OpenSearch service, then call helpers.bulk to upload the remaining documents. This should take around 1 minute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.6 ms, sys: 0 ns, total: 37.6 ms\n",
      "Wall time: 2.68 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46, [])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "es = OpenSearch(\n",
    "    hosts = [{'host': es_endpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "helpers.bulk(es, gendata(0,mergedDf.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Kibana Dashboard\n",
    "In this section, you will create a Kibana Dashboard to display and filter the results.\n",
    "\n",
    "First, grab the url for the Kibana dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://search-nlp-lab-fptm35yk3nbdsp6t4lrhsivzdu.us-east-1.es.amazonaws.com/_plugin/kibana\n"
     ]
    }
   ],
   "source": [
    "print(f'https://{es_endpoint}/_plugin/kibana')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Navigate to the kibana URL printed from the previous cell.\n",
    "\n",
    "2) Once the page loads, select Dashboard.\n",
    "\n",
    "3) Since this is the first time the dashboard is loaded, an Index Pattern will need to be defined. Select Create index pattern.\n",
    "\n",
    "4) Enter movie* as the index pattern name. You should see that the index pattern matches 1 source.\n",
    "\n",
    "5) Choose 'Next step'.\n",
    "\n",
    "6) Choose Create index pattern.\n",
    "\n",
    "7) You should see a table of fields displayed. If everything is working, you will see 28 fields.\n",
    "\n",
    "8) Choose the hamburger menu, and select 'Discover' from the list.\n",
    "\n",
    "9) In the available field list on the left, move to the name field and choose add when it appears.\n",
    "\n",
    "10) Choose Save.\n",
    "\n",
    "11) Enter movies as the title and choose `Save'.\n",
    "\n",
    "12) Choose the hamburger menu, and select 'Dashboard' from the list.\n",
    "\n",
    "13) Choose Create new dashboard.\n",
    "\n",
    "14) Choose Add.\n",
    "\n",
    "15) Select Movies from the list.\n",
    "\n",
    "16) Close the Add Panels pane.\n",
    "\n",
    "17) Choose Create New.\n",
    "\n",
    "18) Select Tag Cloud from the list of Visualizations.\n",
    "\n",
    "19) Choose movie* as the source.\n",
    "\n",
    "20) Under Buckets select Add, then choose Tags.\n",
    "\n",
    "21) Choose Terms as the Aggregation.\n",
    "\n",
    "22) Choose keyphrases.Text.keyword as the field.\n",
    "\n",
    "23) Enter 25 as the size.\n",
    "\n",
    "24) Select Update.\n",
    "\n",
    "25) Select Save.\n",
    "\n",
    "26) Enter Key Phrases as the Title.\n",
    "\n",
    "27) Choose Save and return\n",
    "\n",
    "28) Repeat steps 16-26 for the following fields:\n",
    "\n",
    "    <location.Text.keyword>\n",
    "    <organization.Text.keyword>\n",
    "29) Choose 'Create new'.\n",
    "\n",
    "30) Select Metric from the list of Visualizations.\n",
    "\n",
    "31) Choose movie* as the source.\n",
    "\n",
    "32) Select Save\n",
    "\n",
    "33) Enter Total Documents as the Title.\n",
    "\n",
    "34) Choose Save and return\n",
    "\n",
    "35) Select the calendar icon.\n",
    "\n",
    "36) From the Commonly used list, select Today.\n",
    "\n",
    "37) Select the calendar icon again and update the Refresh every to 5 seconds.\n",
    "\n",
    "38) Choose Start.\n",
    "\n",
    "39) Choose Save\n",
    "\n",
    "40) Enter Movies as the title.\n",
    "\n",
    "41) Choose Save\n",
    "\n",
    "With the dashboard created, you can proceed to upload the remaining documents. There are some helper functions that allow you to do this quickly. First define a function that will create the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "Once you have finished experimenting with OpenSearch, you can shutdown the cluster using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = es_client.delete_elasticsearch_domain(\n",
    "    DomainName='nlp-lab'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenSearch typically takes around 10 minutes to complete. While that is happening you can explore some other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed this lab, and you can now end the lab by following the lab guide instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*©2023 Amazon Web Services, Inc. or its affiliates. All rights reserved. This work may not be reproduced or redistributed, in whole or in part, without prior written permission from Amazon Web Services, Inc. Commercial copying, lending, or selling is prohibited. All trademarks are the property of their owners.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b71a13339a0be9489ff337af97259fe0ed71e682663adc836bae31ac651d564e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
